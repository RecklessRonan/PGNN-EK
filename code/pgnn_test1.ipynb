{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import RobertaTokenizer\n",
    "import javalang\n",
    "from javalang.ast import Node\n",
    "from anytree import AnyNode\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn.glob import GlobalAttention\n",
    "from torch_geometric.nn import MessagePassing, GatedGraphConv, GCNConv, global_mean_pool\n",
    "import yaml"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_func = '''public int getLarger(int a, int b) {\n",
    "\t\ta = Math.abs(a);\n",
    "\t\tb = Math.abs(b);\n",
    "\t\tif(a > b) {\n",
    "\t\t\treturn a;\n",
    "\t\t}else {\n",
    "\t\t\treturn b;\n",
    "\t\t}\n",
    "\t}\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "checkpoint = 'microsoft/codebert-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(checkpoint)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer.tokenize('getLarger')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "javalang_special_tokens = ['CompilationUnit','Import','Documented','Declaration','TypeDeclaration','PackageDeclaration',\n",
    "                            'ClassDeclaration','EnumDeclaration','InterfaceDeclaration','AnnotationDeclaration','Type',\n",
    "                            'BasicType','ReferenceType','TypeArgument','TypeParameter','Annotation','ElementValuePair',\n",
    "                            'ElementArrayValue','Member','MethodDeclaration','FieldDeclaration','ConstructorDeclaration',\n",
    "                            'ConstantDeclaration','ArrayInitializer','VariableDeclaration','LocalVariableDeclaration',\n",
    "                            'VariableDeclarator','FormalParameter','InferredFormalParameter','Statement','IfStatement',\n",
    "                            'WhileStatement','DoStatement','ForStatement','AssertStatement','BreakStatement','ContinueStatement',\n",
    "                            'ReturnStatement','ThrowStatement','SynchronizedStatement','TryStatement','SwitchStatement',\n",
    "                            'BlockStatement','StatementExpression','TryResource','CatchClause','CatchClauseParameter',\n",
    "                            'SwitchStatementCase','ForControl','EnhancedForControl','Expression','Assignment','TernaryExpression',\n",
    "                            'BinaryOperation','Cast','MethodReference','LambdaExpression','Primary','Literal','This',\n",
    "                            'MemberReference','Invocation','ExplicitConstructorInvocation','SuperConstructorInvocation',\n",
    "                            'MethodInvocation','SuperMethodInvocation','SuperMemberReference','ArraySelector','ClassReference',\n",
    "                            'VoidClassReference','Creator','ArrayCreator','ClassCreator','InnerClassCreator','EnumBody',\n",
    "                            'EnumConstantDeclaration','AnnotationMethod', 'Modifier']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "special_tokens_dict = {'additional_special_tokens': javalang_special_tokens}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_added_toks"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer.vocab_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# use javalang to generate ASTs and depth-first traverse to generate ast nodes corpus\n",
    "def get_token(node):\n",
    "    token = 'None'\n",
    "    if isinstance(node, str):\n",
    "        token = node\n",
    "    elif isinstance(node, set):\n",
    "        token = 'Modifier'\n",
    "    elif isinstance(node, Node):\n",
    "        token = node.__class__.__name__\n",
    "    return token\n",
    "\n",
    "\n",
    "def get_child(root):\n",
    "    if isinstance(root, Node):\n",
    "        children = root.children\n",
    "    elif isinstance(root, set):\n",
    "        children = list(root)\n",
    "    else:\n",
    "        children = []\n",
    "\n",
    "    def expand(nested_list):\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, list):\n",
    "                for sub_item in expand(item):\n",
    "                    yield sub_item\n",
    "            elif item:\n",
    "                yield item\n",
    "\n",
    "    return list(expand(children))\n",
    "\n",
    "\n",
    "def get_sequence(node, sequence):\n",
    "    token, children = get_token(node), get_child(node)\n",
    "    sequence.append(token)\n",
    "    for child in children:\n",
    "        get_sequence(child, sequence)\n",
    "\n",
    "\n",
    "def parse_program(func):\n",
    "    tokens = javalang.tokenizer.tokenize(func)\n",
    "    parser = javalang.parser.Parser(tokens)\n",
    "    tree = parser.parse_member_declaration()\n",
    "    return tree"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  generate tree for AST Node\n",
    "def create_tree(root, node, node_list, sub_id_list, leave_list, tokenizer, parent=None):\n",
    "    id = len(node_list)\n",
    "    node_list.append(node)\n",
    "    token, children = get_token(node), get_child(node)\n",
    "\n",
    "    if children == []:\n",
    "        # print('this is a leaf:', token, id)\n",
    "        leave_list.append(id)\n",
    "\n",
    "    # Use roberta.tokenizer to generate subtokens\n",
    "    # If a token can be divided into multiple(>1) subtokens, the first subtoken will be set as the previous node, \n",
    "    # and the other subtokens will be set as its new children   \n",
    "    sub_token_list = tokenizer.tokenize(token)\n",
    "    \n",
    "    if id == 0:\n",
    "        root.token = sub_token_list[0] # the root node is one of the tokenizer's special tokens\n",
    "        root.data = node\n",
    "        # record the num of nodes for every children of root\n",
    "        root_children_node_num = []\n",
    "        for child in children:\n",
    "            node_num = len(node_list)\n",
    "            create_tree(root, child, node_list, sub_id_list, leave_list, tokenizer, parent=root)\n",
    "            root_children_node_num.append(len(node_list) - node_num)        \n",
    "        return root_children_node_num\n",
    "    else:\n",
    "        # print(sub_token_list)\n",
    "        new_node = AnyNode(id=id, token=sub_token_list[0], data=node, parent=parent)\n",
    "        if len(sub_token_list) > 1:\n",
    "            sub_id_list.append(id)\n",
    "            for sub_token in sub_token_list[1:]:\n",
    "                id += 1\n",
    "                AnyNode(id=id, token=sub_token, data=node, parent=new_node)\n",
    "                node_list.append(sub_token)\n",
    "                sub_id_list.append(id)\n",
    "        \n",
    "        for child in children:\n",
    "            create_tree(root, child, node_list, sub_id_list, leave_list, tokenizer, parent=new_node)\n",
    "    # print(token, id)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# traverse the AST tree to get all the nodes and edges\n",
    "def get_node_and_edge(node, node_index_list, tokenizer, src, tgt, variable_token_list, variable_id_list):\n",
    "    token = node.token\n",
    "    # print('id', node.id, 'token', node.token)\n",
    "    node_index_list.append(tokenizer.convert_tokens_to_ids(token))\n",
    "    # node_index_list.append([vocab_dict.word2id.get(token, UNK)])\n",
    "    # find out all variables\n",
    "    if token in ['VariableDeclarator', 'MemberReference']:\n",
    "        variable_token_list.append(node.children[0].token)\n",
    "        variable_id_list.append(node.children[0].id)   \n",
    "    for child in node.children:\n",
    "        src.append(node.id)\n",
    "        tgt.append(child.id)\n",
    "        src.append(child.id)\n",
    "        tgt.append(node.id)\n",
    "        get_node_and_edge(child, node_index_list, tokenizer, src, tgt, variable_token_list, variable_id_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# generate pytorch_geometric input format data from ast\n",
    "def get_pyg_data_from_ast(ast, tokenizer):\n",
    "    node_list = []\n",
    "    sub_id_list = [] # record the ids of node that can be divide into multple subtokens\n",
    "    leave_list = [] # record the ids of leave \n",
    "    new_tree = AnyNode(id=0, token=None, data=None)\n",
    "    root_children_node_num = create_tree(new_tree, ast, node_list, sub_id_list, leave_list, tokenizer)\n",
    "    # print('root_children_node_num', root_children_node_num)\n",
    "    x = []\n",
    "    edge_src = []\n",
    "    edge_tgt = []\n",
    "    # record variable tokens and ids to add data flow edge in AST graph\n",
    "    variable_token_list = []\n",
    "    variable_id_list = []\n",
    "    get_node_and_edge(new_tree, x, tokenizer, edge_src, edge_tgt, variable_token_list, variable_id_list)\n",
    "\n",
    "    ast_edge_num = len(edge_src)\n",
    "    edge_attr = [[0] for _ in range(ast_edge_num)]\n",
    "    # set subtoken edge type to 2\n",
    "    for i in range(len(edge_attr)):\n",
    "        if edge_src[i] in sub_id_list and edge_tgt[i] in sub_id_list:\n",
    "            edge_attr[i] = [2]\n",
    "    # add data flow edge\n",
    "    variable_dict = {}\n",
    "    for i in range(len(variable_token_list)):\n",
    "        # print('variable_dict', variable_dict)\n",
    "        if variable_token_list[i] not in variable_dict:\n",
    "            variable_dict.setdefault(variable_token_list[i], variable_id_list[i])\n",
    "        else:\n",
    "            # print('edge', variable_dict.get(variable_token_list[i]), variable_id_list[i])\n",
    "            edge_src.append(variable_dict.get(variable_token_list[i]))\n",
    "            edge_tgt.append(variable_id_list[i])\n",
    "            edge_src.append(variable_id_list[i])\n",
    "            edge_tgt.append(variable_dict.get(variable_token_list[i]))\n",
    "            variable_dict[variable_token_list[i]] = variable_id_list[i]\n",
    "    dataflow_edge_num = len(edge_src) - ast_edge_num\n",
    "\n",
    "    # add next-token edge\n",
    "    nexttoken_edge_num = len(leave_list)-1\n",
    "    for i in range(nexttoken_edge_num):\n",
    "        edge_src.append(leave_list[i])\n",
    "        edge_tgt.append(leave_list[i+1])\n",
    "        edge_src.append(leave_list[i+1])\n",
    "        edge_tgt.append(leave_list[i])\n",
    "\n",
    "    edge_index = [edge_src, edge_tgt]\n",
    "\n",
    "    # set data flow edge type to 1\n",
    "    for _ in range(dataflow_edge_num):\n",
    "        edge_attr.append([1])\n",
    "    \n",
    "    # set data flow edge type to 3\n",
    "    for _ in range(nexttoken_edge_num * 2):\n",
    "        edge_attr.append([3])\n",
    "    \n",
    "    return x, edge_index, edge_attr, root_children_node_num"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x, edge_index, edge_attr, root_children_node_num = get_pyg_data_from_ast(ast=parse_program(func=test_func), tokenizer=tokenizer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "root_children_node_num"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "divide_node_num = 10\n",
    "max_subgraph_num = int(100/divide_node_num)\n",
    "\n",
    "def get_subgraph_node_num(root_children_node_num, divide_node_num):\n",
    "    subgraph_node_num = []\n",
    "    node_sum = 0\n",
    "    real_graph_num = 0\n",
    "    for num in root_children_node_num:\n",
    "        node_sum += num\n",
    "        if node_sum >= divide_node_num:\n",
    "            subgraph_node_num.append(node_sum)\n",
    "            node_sum = 0    \n",
    "    real_graph_num = len(subgraph_node_num)\n",
    "\n",
    "    # if the last subgraph node num < divide_node_num, then put the last subgraph to the second to last subgraph\n",
    "    if subgraph_node_num[-1] < divide_node_num:\n",
    "        subgraph_node_num[-2] = subgraph_node_num[-2] + subgraph_node_num[-1]\n",
    "        subgraph_node_num[-1] = 0\n",
    "        real_graph_num -= 1\n",
    "\n",
    "    # zero padding for tensor transforming\n",
    "    for _ in range(real_graph_num, max_subgraph_num):\n",
    "        subgraph_node_num.append(0)\n",
    "    \n",
    "    return subgraph_node_num, real_graph_num"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "subgraph_node_num, real_graph_num = get_subgraph_node_num(root_children_node_num, divide_node_num)\n",
    "subgraph_node_num"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "real_graph_num"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = Data(\n",
    "    x = torch.tensor(x, dtype=torch.long),\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long),\n",
    "    edge_attr=torch.tensor(edge_attr, dtype=torch.long),\n",
    "    subgraph_node_num=torch.tensor(subgraph_node_num, dtype=torch.long),\n",
    "    real_graph_num=torch.tensor(real_graph_num, dtype=torch.long)\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_list = [data for _ in range(32)]\n",
    "loader = DataLoader(data_list, batch_size=32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch = next(iter(loader))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class SequenceGNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_len, graph_embedding_size, gnn_layers_num, lstm_layers_num, lstm_hidden_size, divide_node_num,\n",
    "                    decoder_input_size, device):\n",
    "        super(SequenceGNNEncoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.embed = nn.Embedding(vocab_len, graph_embedding_size)\n",
    "        self.edge_embed = nn.Embedding(4, 1) # only two edge types to be set weights, which are AST edge and data flow edge\n",
    "        self.ggnnlayer = GatedGraphConv(graph_embedding_size, gnn_layers_num)\n",
    "        self.mlp_gate = nn.Sequential(\n",
    "            nn.Linear(graph_embedding_size, 300), nn.Sigmoid(), nn.Linear(300, 1), nn.Sigmoid())\n",
    "        self.pool = GlobalAttention(gate_nn=self.mlp_gate)\n",
    "        self.divide_node_num = divide_node_num\n",
    "        self.lstm = nn.LSTM(input_size=graph_embedding_size, hidden_size=lstm_hidden_size, num_layers=lstm_layers_num)\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_layers_num = lstm_layers_num\n",
    "        self.fc = nn.Linear(graph_embedding_size + lstm_hidden_size, decoder_input_size)\n",
    "\n",
    "    def subgraph_forward(self, x, edge_index, edge_attr, batch):\n",
    "        if type(edge_attr) == type(None):\n",
    "            edge_weight = None\n",
    "        else:\n",
    "            edge_weight = self.edge_embed(edge_attr)\n",
    "            edge_weight = edge_weight.squeeze(1)\n",
    "        x = self.ggnnlayer(x, edge_index, edge_weight)\n",
    "        return self.pool(x, batch=batch)\n",
    "    \n",
    "    # partitioning multiple subgraphs by dynamic allocating edges\n",
    "    def partition_graph(self, x, edge_index, edge_attr, subgraph_node_num, real_graph_num, ptr):        \n",
    "        nodes_list = [] # record all nodes number for each subgraph in total batch\n",
    "        subgraph_num = max(real_graph_num)\n",
    "\n",
    "        batch_size = subgraph_node_num.size(0)\n",
    "        start_node_num = [1 for _ in range(batch_size)]\n",
    "        for i in range(subgraph_num):\n",
    "            subgraph_nodes_list = []\n",
    "            for j in range(batch_size):\n",
    "                if subgraph_node_num[j][i] != 0:\n",
    "                    for k in range(ptr[j]+start_node_num[j], ptr[j]+start_node_num[j]+subgraph_node_num[j][i]):\n",
    "                        subgraph_nodes_list.append(k)\n",
    "                    start_node_num[j] += subgraph_node_num[j][i]\n",
    "            nodes_list.append(subgraph_nodes_list)\n",
    "\n",
    "        # only count the edge whose target node in subgraph\n",
    "        sub_edge_src = [[] for _ in range(subgraph_num)]\n",
    "        sub_edge_tgt = [[] for _ in range(subgraph_num)]\n",
    "        sub_edge_attr = [[] for _ in range(subgraph_num)]\n",
    "        # print('nodes_list', nodes_list)\n",
    "        node_num = len(x)\n",
    "        node_subgraph_index = [0 for _ in range(node_num)] # use a list to store the subgraph numbers for all nodes\n",
    "        for i in range(len(nodes_list)):\n",
    "            for node in nodes_list[i]:\n",
    "                node_subgraph_index[node] = i\n",
    "\n",
    "        for i in range(len(edge_index[1])):\n",
    "            src = edge_index[0][i].item()\n",
    "            tgt = edge_index[1][i].item()\n",
    "            sub_edge_src[node_subgraph_index[tgt]].append(src)\n",
    "            sub_edge_tgt[node_subgraph_index[tgt]].append(tgt)\n",
    "            sub_edge_attr[node_subgraph_index[tgt]].append(edge_attr[i].item())\n",
    "        edge_index_list = []\n",
    "        edge_attr_list = []\n",
    "        for i in range(subgraph_num):\n",
    "            edge_index_list.append(torch.tensor([sub_edge_src[i], sub_edge_tgt[i]], dtype=torch.long))\n",
    "            edge_attr_list.append(torch.tensor(sub_edge_attr[i], dtype=torch.long))\n",
    "        print('nodes_list', nodes_list)\n",
    "        return edge_index_list, edge_attr_list  \n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, subgraph_node_num, real_graph_num, batch, ptr):\n",
    "        edge_index_list, edge_attr_list = self.partition_graph(x, edge_index, edge_attr, subgraph_node_num, real_graph_num, ptr)\n",
    "        print('edge_index_list', edge_index_list)\n",
    "        print('edge_attr_list', edge_attr_list)\n",
    "        x = self.embed(x)\n",
    "        x = x.squeeze(1)\n",
    "        subgraph_pool_list = [\n",
    "            self.subgraph_forward(x, edge_index_list[i].to(self.device), edge_attr_list[i].to(self.device), batch)\n",
    "            for i in range(len(edge_index_list))\n",
    "        ]\n",
    "        graph_pool = self.subgraph_forward(x, edge_index, edge_attr, batch)\n",
    "        print('graph_pool', graph_pool.shape)\n",
    "        subgraph_pool_seq = torch.stack(subgraph_pool_list)\n",
    "        print('subgraph_pool_seq', subgraph_pool_seq.shape)\n",
    "        h0 = torch.zeros(self.lstm_layers_num, subgraph_pool_seq.size(1) ,self.lstm_hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.lstm_layers_num, subgraph_pool_seq.size(1) ,self.lstm_hidden_size).to(self.device)\n",
    "        subgraph_output, (_, _) = self.lstm(subgraph_pool_seq, (h0, c0))\n",
    "        return self.fc(torch.cat((subgraph_output[-1], graph_pool), dim=1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "config_file = 'config_dgnn.yml'\n",
    "config = yaml.load(open(config_file), Loader=yaml.FullLoader)\n",
    "\n",
    "# data source\n",
    "TRAIN_DIR = config['middle_data']['train']\n",
    "VALID_DIR = config['middle_data']['valid']\n",
    "TEST_DIR = config['middle_data']['test']\n",
    "\n",
    "\n",
    "# training parameter\n",
    "batch_size = config['training']['batch_size']\n",
    "num_epoches = config['training']['num_epoches']\n",
    "lr = config['training']['lr']\n",
    "decay_ratio = config['training']['lr']\n",
    "save_name = config['training']['save_name']\n",
    "warm_up = config['training']['warm_up']\n",
    "patience = config['training']['patience']\n",
    "\n",
    "# model design\n",
    "graph_embedding_size = config['model']['graph_embedding_size']\n",
    "lstm_hidden_size = config['model']['lstm_hidden_size']\n",
    "# divide_node_num = config['model']['divide_node_num']\n",
    "gnn_layers_num = config['model']['gnn_layers_num']\n",
    "lstm_layers_num = config['model']['lstm_layers_num']\n",
    "decoder_input_size = config['model']['decoder_input_size']\n",
    "decoder_hidden_size = config['model']['decoder_hidden_size']\n",
    "decoder_num_layers = config['model']['decoder_num_layers']\n",
    "decoder_rnn_dropout = config['model']['decoder_rnn_dropout']\n",
    "\n",
    "# logs\n",
    "info_prefix = config['logs']['info_prefix']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "model_args = {\n",
    "    'vocab_len': 51000,\n",
    "    'graph_embedding_size': graph_embedding_size,\n",
    "    'gnn_layers_num': gnn_layers_num,\n",
    "    'lstm_layers_num': lstm_layers_num,\n",
    "    'lstm_hidden_size': lstm_hidden_size,\n",
    "    'divide_node_num': divide_node_num,\n",
    "    'decoder_input_size': decoder_input_size,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "model = SequenceGNNEncoder(**model_args).to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch = batch.to(device)\n",
    "subgraph_node_num = torch.stack(torch.split(batch.subgraph_node_num, max_subgraph_num))\n",
    "subgraph_node_num"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "real_graph_num = torch.stack(torch.split(batch.real_graph_num, 1))\n",
    "real_graph_num"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max(real_graph_num)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch.ptr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model(batch.x, batch.edge_index, batch.edge_attr, subgraph_node_num, real_graph_num, batch.batch, batch.ptr).shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use regex to match all special tokens in Javalang"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ast_node = '''\n",
    "from .ast import Node\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class CompilationUnit(Node):\n",
    "    attrs = (\"package\", \"imports\", \"types\")\n",
    "\n",
    "class Import(Node):\n",
    "    attrs = (\"path\", \"static\", \"wildcard\")\n",
    "\n",
    "class Documented(Node):\n",
    "    attrs = (\"documentation\",)\n",
    "\n",
    "class Declaration(Node):\n",
    "    attrs = (\"modifiers\", \"annotations\")\n",
    "\n",
    "class TypeDeclaration(Declaration, Documented):\n",
    "    attrs = (\"name\", \"body\")\n",
    "\n",
    "    @property\n",
    "    def fields(self):\n",
    "        return [decl for decl in self.body if isinstance(decl, FieldDeclaration)]\n",
    "\n",
    "    @property\n",
    "    def methods(self):\n",
    "        return [decl for decl in self.body if isinstance(decl, MethodDeclaration)]\n",
    "\n",
    "    @property\n",
    "    def constructors(self):\n",
    "        return [decl for decl in self.body if isinstance(decl, ConstructorDeclaration)]\n",
    "\n",
    "class PackageDeclaration(Declaration, Documented):\n",
    "    attrs = (\"name\",)\n",
    "\n",
    "class ClassDeclaration(TypeDeclaration):\n",
    "    attrs = (\"type_parameters\", \"extends\", \"implements\")\n",
    "\n",
    "class EnumDeclaration(TypeDeclaration):\n",
    "    attrs = (\"implements\",)\n",
    "\n",
    "    @property\n",
    "    def fields(self):\n",
    "        return [decl for decl in self.body.declarations if isinstance(decl, FieldDeclaration)]\n",
    "\n",
    "    @property\n",
    "    def methods(self):\n",
    "        return [decl for decl in self.body.declarations if isinstance(decl, MethodDeclaration)]\n",
    "\n",
    "class InterfaceDeclaration(TypeDeclaration):\n",
    "    attrs = (\"type_parameters\", \"extends\",)\n",
    "\n",
    "class AnnotationDeclaration(TypeDeclaration):\n",
    "    attrs = ()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class Type(Node):\n",
    "    attrs = (\"name\", \"dimensions\",)\n",
    "\n",
    "class BasicType(Type):\n",
    "    attrs = ()\n",
    "\n",
    "class ReferenceType(Type):\n",
    "    attrs = (\"arguments\", \"sub_type\")\n",
    "\n",
    "class TypeArgument(Node):\n",
    "    attrs = (\"type\", \"pattern_type\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class TypeParameter(Node):\n",
    "    attrs = (\"name\", \"extends\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class Annotation(Node):\n",
    "    attrs = (\"name\", \"element\")\n",
    "\n",
    "class ElementValuePair(Node):\n",
    "    attrs = (\"name\", \"value\")\n",
    "\n",
    "class ElementArrayValue(Node):\n",
    "    attrs = (\"values\",)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class Member(Documented):\n",
    "    attrs = ()\n",
    "\n",
    "class MethodDeclaration(Member, Declaration):\n",
    "    attrs = (\"type_parameters\", \"return_type\", \"name\", \"parameters\", \"throws\", \"body\")\n",
    "\n",
    "class FieldDeclaration(Member, Declaration):\n",
    "    attrs = (\"type\", \"declarators\")\n",
    "\n",
    "class ConstructorDeclaration(Declaration, Documented):\n",
    "    attrs = (\"type_parameters\", \"name\", \"parameters\", \"throws\", \"body\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class ConstantDeclaration(FieldDeclaration):\n",
    "    attrs = ()\n",
    "\n",
    "class ArrayInitializer(Node):\n",
    "    attrs = (\"initializers\",)\n",
    "\n",
    "class VariableDeclaration(Declaration):\n",
    "    attrs = (\"type\", \"declarators\")\n",
    "\n",
    "class LocalVariableDeclaration(VariableDeclaration):\n",
    "    attrs = ()\n",
    "\n",
    "class VariableDeclarator(Node):\n",
    "    attrs = (\"name\", \"dimensions\", \"initializer\")\n",
    "\n",
    "class FormalParameter(Declaration):\n",
    "    attrs = (\"type\", \"name\", \"varargs\")\n",
    "\n",
    "class InferredFormalParameter(Node):\n",
    "    attrs = ('name',)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class Statement(Node):\n",
    "    attrs = (\"label\",)\n",
    "\n",
    "class IfStatement(Statement):\n",
    "    attrs = (\"condition\", \"then_statement\", \"else_statement\")\n",
    "\n",
    "class WhileStatement(Statement):\n",
    "    attrs = (\"condition\", \"body\")\n",
    "\n",
    "class DoStatement(Statement):\n",
    "    attrs = (\"condition\", \"body\")\n",
    "\n",
    "class ForStatement(Statement):\n",
    "    attrs = (\"control\", \"body\")\n",
    "\n",
    "class AssertStatement(Statement):\n",
    "    attrs = (\"condition\", \"value\")\n",
    "\n",
    "class BreakStatement(Statement):\n",
    "    attrs = (\"goto\",)\n",
    "\n",
    "class ContinueStatement(Statement):\n",
    "    attrs = (\"goto\",)\n",
    "\n",
    "class ReturnStatement(Statement):\n",
    "    attrs = (\"expression\",)\n",
    "\n",
    "class ThrowStatement(Statement):\n",
    "    attrs = (\"expression\",)\n",
    "\n",
    "class SynchronizedStatement(Statement):\n",
    "    attrs = (\"lock\", \"block\")\n",
    "\n",
    "class TryStatement(Statement):\n",
    "    attrs = (\"resources\", \"block\", \"catches\", \"finally_block\")\n",
    "\n",
    "class SwitchStatement(Statement):\n",
    "    attrs = (\"expression\", \"cases\")\n",
    "\n",
    "class BlockStatement(Statement):\n",
    "    attrs = (\"statements\",)\n",
    "\n",
    "class StatementExpression(Statement):\n",
    "    attrs = (\"expression\",)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class TryResource(Declaration):\n",
    "    attrs = (\"type\", \"name\", \"value\")\n",
    "\n",
    "class CatchClause(Statement):\n",
    "    attrs = (\"parameter\", \"block\")\n",
    "\n",
    "class CatchClauseParameter(Declaration):\n",
    "    attrs = (\"types\", \"name\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class SwitchStatementCase(Node):\n",
    "    attrs = (\"case\", \"statements\")\n",
    "\n",
    "class ForControl(Node):\n",
    "    attrs = (\"init\", \"condition\", \"update\")\n",
    "\n",
    "class EnhancedForControl(Node):\n",
    "    attrs = (\"var\", \"iterable\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class Expression(Node):\n",
    "    attrs = ()\n",
    "\n",
    "class Assignment(Expression):\n",
    "    attrs = (\"expressionl\", \"value\", \"type\")\n",
    "\n",
    "class TernaryExpression(Expression):\n",
    "    attrs = (\"condition\", \"if_true\", \"if_false\")\n",
    "\n",
    "class BinaryOperation(Expression):\n",
    "    attrs = (\"operator\", \"operandl\", \"operandr\")\n",
    "\n",
    "class Cast(Expression):\n",
    "    attrs = (\"type\", \"expression\")\n",
    "\n",
    "class MethodReference(Expression):\n",
    "    attrs = (\"expression\", \"method\", \"type_arguments\")\n",
    "\n",
    "class LambdaExpression(Expression):\n",
    "    attrs = ('parameters', 'body')\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class Primary(Expression):\n",
    "    attrs = (\"prefix_operators\", \"postfix_operators\", \"qualifier\", \"selectors\")\n",
    "\n",
    "class Literal(Primary):\n",
    "    attrs = (\"value\",)\n",
    "\n",
    "class This(Primary):\n",
    "    attrs = ()\n",
    "\n",
    "class MemberReference(Primary):\n",
    "    attrs = (\"member\",)\n",
    "\n",
    "class Invocation(Primary):\n",
    "    attrs = (\"type_arguments\", \"arguments\")\n",
    "\n",
    "class ExplicitConstructorInvocation(Invocation):\n",
    "    attrs = ()\n",
    "\n",
    "class SuperConstructorInvocation(Invocation):\n",
    "    attrs = ()\n",
    "\n",
    "class MethodInvocation(Invocation):\n",
    "    attrs = (\"member\",)\n",
    "\n",
    "class SuperMethodInvocation(Invocation):\n",
    "    attrs = (\"member\",)\n",
    "\n",
    "class SuperMemberReference(Primary):\n",
    "    attrs = (\"member\",)\n",
    "\n",
    "class ArraySelector(Expression):\n",
    "    attrs = (\"index\",)\n",
    "\n",
    "class ClassReference(Primary):\n",
    "    attrs = (\"type\",)\n",
    "\n",
    "class VoidClassReference(ClassReference):\n",
    "    attrs = ()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class Creator(Primary):\n",
    "    attrs = (\"type\",)\n",
    "\n",
    "class ArrayCreator(Creator):\n",
    "    attrs = (\"dimensions\", \"initializer\")\n",
    "\n",
    "class ClassCreator(Creator):\n",
    "    attrs = (\"constructor_type_arguments\", \"arguments\", \"body\")\n",
    "\n",
    "class InnerClassCreator(Creator):\n",
    "    attrs = (\"constructor_type_arguments\", \"arguments\", \"body\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class EnumBody(Node):\n",
    "    attrs = (\"constants\", \"declarations\")\n",
    "\n",
    "class EnumConstantDeclaration(Declaration, Documented):\n",
    "    attrs = (\"name\", \"arguments\", \"body\")\n",
    "\n",
    "class AnnotationMethod(Declaration):\n",
    "    attrs = (\"name\", \"return_type\", \"dimensions\", \"default\")\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "result = re.findall('.*class (.*)\\(.*', ast_node)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'\\',\\''.join(result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# try:   \n",
    "#     !jupyter nbconvert --to python subword_test.ipynb\n",
    "#     # python即转化为.py，script即转化为.html\n",
    "#     # file_name.ipynb即当前module的文件名\n",
    "# except:\n",
    "#     pass\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}