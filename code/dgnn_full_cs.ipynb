{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import yaml\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd \n",
    "import javalang\n",
    "from javalang.ast import Node\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch_geometric.nn.glob import GlobalAttention\n",
    "from torch_geometric.nn import MessagePassing, GatedGraphConv, GCNConv, global_mean_pool\n",
    "from anytree import AnyNode\n",
    "from torch_geometric.data import Data, DataLoader, ClusterData, ClusterLoader\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from ignite.metrics.nlp import Bleu\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Paramerters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "config_file = 'config_dgnn.yml'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "config = yaml.load(open(config_file), Loader=yaml.FullLoader)\n",
    "\n",
    "# data source\n",
    "TRAIN_DIR = config['data']['train']\n",
    "VALID_DIR = config['data']['valid']\n",
    "TEST_DIR = config['data']['test']\n",
    "\n",
    "\n",
    "# prepocess design\n",
    "# max_seq_len = config['preprocess']['max_seq_len']\n",
    "\n",
    "# training parameter\n",
    "batch_size = config['training']['batch_size']\n",
    "num_epoches = config['training']['num_epoches']\n",
    "lr = config['training']['lr']\n",
    "decay_ratio = config['training']['lr']\n",
    "save_name = config['training']['save_name']\n",
    "warm_up = config['training']['warm_up']\n",
    "patience = config['training']['patience']\n",
    "\n",
    "# model design\n",
    "graph_embedding_size = config['model']['graph_embedding_size']\n",
    "lstm_hidden_size = config['model']['lstm_hidden_size']\n",
    "divide_node_num = config['model']['divide_node_num']\n",
    "gnn_layers_num = config['model']['gnn_layers_num']\n",
    "lstm_layers_num = config['model']['lstm_layers_num']\n",
    "decoder_input_size = config['model']['decoder_input_size']\n",
    "decoder_hidden_size = config['model']['decoder_hidden_size']\n",
    "decoder_num_layers = config['model']['decoder_num_layers']\n",
    "decoder_rnn_dropout = config['model']['decoder_rnn_dropout']\n",
    "\n",
    "# logs\n",
    "info_prefix = config['logs']['info_prefix']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Logs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run_id = datetime.now().strftime('%Y-%m-%d--%H-%M-%S')\n",
    "log_file = 'logs/' + run_id + '.log'\n",
    "exp_dir = 'runs/' + run_id\n",
    "os.mkdir(exp_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Info(object):\n",
    "    def __init__(self, info_prefix=''):\n",
    "        self.info_prefix = info_prefix\n",
    "    \n",
    "    def print_msg(self, msg):\n",
    "        text = self.info_prefix + ' ' + msg\n",
    "        print(text)\n",
    "        logging.info(text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "logging.basicConfig(format='%(asctime)s | %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', filename=log_file, level=logging.DEBUG)\n",
    "msgr = Info(info_prefix)\n",
    "\n",
    "msgr.print_msg('run_id : {}'.format(run_id))\n",
    "msgr.print_msg('log_file : {}'.format(log_file))\n",
    "msgr.print_msg('exp_dir: {}'.format(exp_dir))\n",
    "msgr.print_msg(str(config))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sequence Preprocess"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define four extra keywords \n",
    "PAD_TOKEN = '<PAD>'\n",
    "BOS_TOKEN = '<S>'\n",
    "EOS_TOKEN = '</S>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "PAD = 0\n",
    "BOS = 1\n",
    "EOS = 2\n",
    "UNK = 3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# read dataset\n",
    "train_data = pd.read_json(path_or_buf=TRAIN_DIR, lines=True)\n",
    "valid_data = pd.read_json(path_or_buf=VALID_DIR, lines=True)\n",
    "test_data = pd.read_json(path_or_buf=TEST_DIR, lines=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "msgr.print_msg('train size: {}, valid size: {}, test size: {}'.format(len(train_data), len(valid_data), len(test_data)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define vocab class\n",
    "class Vocab(object):\n",
    "    def __init__(self, word2id={}):\n",
    "        self.word2id = dict(word2id)\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "\n",
    "    def build_vocab(self, sentences, min_count=1):\n",
    "        word_counter = {}\n",
    "        for word in sentences:\n",
    "            word_counter[word] = word_counter.get(word, 0) + 1\n",
    "        \n",
    "        for word, count in sorted(word_counter.items(), key=lambda x: -x[1]):\n",
    "            if count < min_count:\n",
    "                break\n",
    "            _id = len(self.word2id)\n",
    "            self.word2id.setdefault(word, _id)\n",
    "            self.id2word[_id] = word"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# construct two vocabulary for ast nodes and natural language repectively\n",
    "word2id = {\n",
    "    PAD_TOKEN: PAD,\n",
    "    BOS_TOKEN: BOS,\n",
    "    EOS_TOKEN: EOS,\n",
    "    UNK_TOKEN: UNK,\n",
    "}\n",
    "\n",
    "vocab_astnodes = Vocab(word2id=word2id)\n",
    "vocab_nl = Vocab(word2id=word2id)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# use `docstring_tokens` in train to generate natural language corpus\n",
    "nl_tokens = []\n",
    "for nl_token in train_data['docstring_tokens']:\n",
    "    nl_tokens.extend(nl_token)\n",
    "\n",
    "# set the all natural language to lowercase\n",
    "for i in range(len(nl_tokens)):\n",
    "    nl_tokens[i] = nl_tokens[i].lower()\n",
    "\n",
    "vocab_nl.build_vocab(nl_tokens, min_count=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vocab_nl_size = len(vocab_nl.id2word)\n",
    "msgr.print_msg('vocab_nl_size: ' + str(vocab_nl_size))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# use javalang to generate ASTs and depth-first traverse to generate ast nodes corpus\n",
    "def get_token(node):\n",
    "    token = ''\n",
    "    if isinstance(node, str):\n",
    "        token = node\n",
    "    elif isinstance(node, set):\n",
    "        token = 'Modifier'\n",
    "    elif isinstance(node, Node):\n",
    "        token = node.__class__.__name__\n",
    "    return token\n",
    "\n",
    "\n",
    "def get_child(root):\n",
    "    if isinstance(root, Node):\n",
    "        children = root.children\n",
    "    elif isinstance(root, set):\n",
    "        children = list(root)\n",
    "    else:\n",
    "        children = []\n",
    "\n",
    "    def expand(nested_list):\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, list):\n",
    "                for sub_item in expand(item):\n",
    "                    yield sub_item\n",
    "            elif item:\n",
    "                yield item\n",
    "\n",
    "    return list(expand(children))\n",
    "\n",
    "\n",
    "def get_sequence(node, sequence):\n",
    "    token, children = get_token(node), get_child(node)\n",
    "    sequence.append(token)\n",
    "    for child in children:\n",
    "        get_sequence(child, sequence)\n",
    "\n",
    "\n",
    "def parse_program(func):\n",
    "    tokens = javalang.tokenizer.tokenize(func)\n",
    "    parser = javalang.parser.Parser(tokens)\n",
    "    tree = parser.parse_member_declaration()\n",
    "    return tree"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# use train data to construction ast nodes corpus\n",
    "astnodes_tokens = []\n",
    "for code in tqdm(train_data['code']):\n",
    "    sequence = []\n",
    "    get_sequence(parse_program(code), sequence)\n",
    "    astnodes_tokens.extend(sequence)\n",
    "\n",
    "vocab_astnodes.build_vocab(astnodes_tokens, min_count=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vocab_astnodes_size = len(vocab_astnodes.id2word)\n",
    "msgr.print_msg('vocab_astnodes_size: ' + str(vocab_astnodes_size))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(set(astnodes_tokens))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(astnodes_tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "astnodes_tokens_set = list(set(astnodes_tokens))\n",
    "len(astnodes_tokens_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "camel_case_split('CamelCaseXYZ')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_tokens = []\n",
    "for token in tqdm(astnodes_tokens_set):\n",
    "    if '_' in token:\n",
    "        all_tokens.extend(token.split('_'))\n",
    "    else:\n",
    "        all_tokens.extend(camel_case_split(token))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(all_tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(set(all_tokens))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# transform sentence to ids\n",
    "def sentence_to_ids(vocab, sentence):\n",
    "    ids = [vocab.word2id.get(word.lower(), UNK) for word in sentence]\n",
    "    ids += [EOS]\n",
    "    return ids\n",
    "\n",
    "# transform ids to sentence\n",
    "def ids_to_sentence(vocab, ids):\n",
    "    return [vocab.id2word[_id] for _id in ids]\n",
    "\n",
    "# pad sequence \n",
    "def pad_seq(seq, max_length):\n",
    "    if len(seq) >= max_length:\n",
    "        return seq[0: max_length]\n",
    "    res = seq + [PAD for i in range(max_length - len(seq))]\n",
    "    return res\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add Dataflow to AST to generate D-AST"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  generate tree for AST Node\n",
    "def create_tree(root, node, node_list, parent=None):\n",
    "    id = len(node_list)\n",
    "    token, children = get_token(node), get_child(node)\n",
    "    if id == 0:\n",
    "        root.token = token\n",
    "        root.data = node\n",
    "    else:\n",
    "        new_node = AnyNode(id=id, token=token, data=node, parent=parent)\n",
    "    node_list.append(node)\n",
    "    for child in children:\n",
    "        if id == 0:\n",
    "            create_tree(root, child, node_list, parent=root)\n",
    "        else:\n",
    "            create_tree(root, child, node_list, parent=new_node)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# traverse the AST tree to get all the nodes and edges\n",
    "def get_node_and_edge(node, node_index_list, vocab_dict, src, tgt, variable_token_list, variable_id_list):\n",
    "    token = node.token\n",
    "    # print('token', token)\n",
    "    node_index_list.append([vocab_dict.word2id.get(token, UNK)])\n",
    "    # find out all variables\n",
    "    if token in ['VariableDeclarator', 'MemberReference']:\n",
    "        variable_token_list.append(node.children[0].token)\n",
    "        variable_id_list.append(node.children[0].id)\n",
    "    for child in node.children:\n",
    "        # print('child', child.token)\n",
    "        src.append(node.id)\n",
    "        tgt.append(child.id)\n",
    "        src.append(child.id)\n",
    "        tgt.append(node.id)\n",
    "        get_node_and_edge(child, node_index_list, vocab_dict, src, tgt, variable_token_list, variable_id_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# generate pytorch_geometric input format data from ast\n",
    "def get_pyg_data_from_ast(ast, vocab_dict):\n",
    "    node_list = []\n",
    "    new_tree = AnyNode(id=0, token=None, data=None)\n",
    "    create_tree(new_tree, ast, node_list)\n",
    "    x = []\n",
    "    edge_src = []\n",
    "    edge_tgt = []\n",
    "    edge_attr = []\n",
    "    # record variable tokens and ids to add data flow edge in AST graph\n",
    "    variable_token_list = []\n",
    "    variable_id_list = []\n",
    "    get_node_and_edge(new_tree, x, vocab_dict, edge_src, edge_tgt, variable_token_list, variable_id_list)\n",
    "    # print('variable_token_list', variable_token_list)\n",
    "    # print('variable_id_list', variable_id_list)\n",
    "\n",
    "    ast_edge_num = len(edge_src)\n",
    "    # print('ast_edge_num', ast_edge_num)\n",
    "    # set ast edge type to 0\n",
    "    for _ in range(ast_edge_num):\n",
    "        edge_attr.append([0])\n",
    "\n",
    "    # add data flow edge\n",
    "    variable_dict = {}\n",
    "    for i in range(len(variable_token_list)):\n",
    "        # print('variable_dict', variable_dict)\n",
    "        if variable_token_list[i] not in variable_dict:\n",
    "            variable_dict.setdefault(variable_token_list[i], variable_id_list[i])\n",
    "        else:\n",
    "            # print('edge', variable_dict.get(variable_token_list[i]), variable_id_list[i])\n",
    "            edge_src.append(variable_dict.get(variable_token_list[i]))\n",
    "            edge_tgt.append(variable_id_list[i])\n",
    "            edge_src.append(variable_id_list[i])\n",
    "            edge_tgt.append(variable_dict.get(variable_token_list[i]))\n",
    "            variable_dict[variable_token_list[i]] = variable_id_list[i]\n",
    "    \n",
    "    edge_index = [edge_src, edge_tgt]\n",
    "\n",
    "    # set data flow edge type to 1\n",
    "    dataflow_edge_num = len(edge_src) - ast_edge_num\n",
    "    for _ in range(dataflow_edge_num):\n",
    "        edge_attr.append([1])\n",
    "    # print('dataflow_edge_num', dataflow_edge_num)\n",
    "    return x, edge_index, edge_attr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Batch Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def transform_to_pygdata(data):\n",
    "    pyg_datas = []\n",
    "    for i in range(len(data)):\n",
    "        ast = parse_program(data['code'][i])\n",
    "        label = sentence_to_ids(vocab_nl, data['docstring_tokens'][i])\n",
    "        x, edge_index, edge_attr = get_pyg_data_from_ast(ast, vocab_astnodes)\n",
    "        pyg_datas.append(\n",
    "            Data(x=torch.tensor(x, dtype=torch.long),\n",
    "                       edge_index=torch.tensor(edge_index, dtype=torch.long),\n",
    "                       edge_attr=torch.tensor(edge_attr, dtype=torch.long),\n",
    "                       y=torch.tensor(pad_seq(label, max_seq_len), dtype=torch.long)),\n",
    "           )\n",
    "    return pyg_datas\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_pygdata = transform_to_pygdata(train_data)\n",
    "valid_pygdata = transform_to_pygdata(valid_data)\n",
    "test_pygdata = transform_to_pygdata(test_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_loader = DataLoader(train_pygdata, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_pygdata, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_pygdata, batch_size=batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# partitioning D-AST in model instead of data-prepocessing\n",
    "# partitioning D-AST in model by the num of nodes, which is set in the hyper-parameter `divide_node_num`\n",
    "# all codes are started with node `MethodDeclation`, and we use it as super-node that kept in all sub graphs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class SequenceGNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_len, graph_embedding_size, gnn_layers_num, lstm_layers_num, lstm_hidden_size, divide_node_num,\n",
    "                    decoder_input_size, device):\n",
    "        super(SequenceGNNEncoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.embed = nn.Embedding(vocab_len, graph_embedding_size, padding_idx=PAD)\n",
    "        self.edge_embed = nn.Embedding(2, 1) # only two edge types to be set weights, which are AST edge and data flow edge\n",
    "        self.ggnnlayer = GatedGraphConv(graph_embedding_size, gnn_layers_num)\n",
    "        self.mlp_gate = nn.Sequential(\n",
    "            nn.Linear(graph_embedding_size, 300), nn.Sigmoid(), nn.Linear(300, 1), nn.Sigmoid())\n",
    "        self.pool = GlobalAttention(gate_nn=self.mlp_gate)\n",
    "        self.divide_node_num = divide_node_num\n",
    "        self.lstm = nn.LSTM(input_size=graph_embedding_size, hidden_size=lstm_hidden_size, num_layers=lstm_layers_num)\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_layers_num = lstm_layers_num\n",
    "        self.fc = nn.Linear(graph_embedding_size + lstm_hidden_size, decoder_input_size)\n",
    "\n",
    "    def subgraph_forward(self, x, edge_index, edge_attr, batch):\n",
    "        if type(edge_attr) == type(None):\n",
    "            edge_weight = None\n",
    "        else:\n",
    "            edge_weight = self.edge_embed(edge_attr)\n",
    "            edge_weight = edge_weight.squeeze(1)\n",
    "        x = self.ggnnlayer(x, edge_index, edge_weight)\n",
    "        return self.pool(x, batch=batch)\n",
    "    \n",
    "    # partitioning multiple subgraphs by dynamic allocating edges\n",
    "    def partition_graph(self, x, edge_index, edge_attr, batch):        \n",
    "        nodes_list = [] # record all nodes number for each subgraph in total batch\n",
    "        graph_pos_in_batch, graph_length = self.get_subgraph_info_from_batch(batch)\n",
    "        max_seq_len = max(graph_length)\n",
    "        subgraph_num = int(max_seq_len/self.divide_node_num) + 1\n",
    "        for i in range(subgraph_num):\n",
    "            nodes = []\n",
    "            for j in range(len(graph_pos_in_batch)):\n",
    "                if graph_length[j] > i * self.divide_node_num:\n",
    "                    if graph_length[j] > (i+1) * self.divide_node_num:\n",
    "                        subgraph_len = self.divide_node_num\n",
    "                    else:\n",
    "                        subgraph_len = graph_length[j] - i * self.divide_node_num   \n",
    "                    for m in range(subgraph_len):\n",
    "                        nodes.append(graph_pos_in_batch[j] + m)          \n",
    "            nodes_list.append(set(nodes)) \n",
    "        # only count the edge whose target node in subgraph\n",
    "        sub_edge_src = [[] for _ in range(subgraph_num)]\n",
    "        sub_edge_tgt = [[] for _ in range(subgraph_num)]\n",
    "        sub_edge_attr = [[] for _ in range(subgraph_num)]\n",
    "        # print('nodes_list', nodes_list)\n",
    "        node_num = len(x)\n",
    "        node_subgraph_index = [0 for _ in range(node_num)] # use a list to store the subgraph numbers for all nodes\n",
    "        for i in range(len(nodes_list)):\n",
    "            for node in nodes_list[i]:\n",
    "                node_subgraph_index[node] = i\n",
    "    \n",
    "        for i in range(len(edge_index[1])):\n",
    "            src = edge_index[0][i].item()\n",
    "            tgt = edge_index[1][i].item()\n",
    "            sub_edge_src[node_subgraph_index[tgt]].append(src)\n",
    "            sub_edge_tgt[node_subgraph_index[tgt]].append(tgt)\n",
    "            sub_edge_attr[node_subgraph_index[tgt]].append(edge_attr[i].item())\n",
    "        edge_index_list = []\n",
    "        edge_attr_list = []\n",
    "        for i in range(subgraph_num):\n",
    "            edge_index_list.append(torch.tensor([sub_edge_src[i], sub_edge_tgt[i]], dtype=torch.long))\n",
    "            edge_attr_list.append(torch.tensor(sub_edge_attr[i], dtype=torch.long))\n",
    "        return edge_index_list, edge_attr_list\n",
    "\n",
    "    def get_subgraph_info_from_batch(self, batch):\n",
    "        comp = 0\n",
    "        pos = 0\n",
    "        graph_pos_in_batch = [0] # record begin positions and end positions of every subgraph\n",
    "        graph_length = [] # use a list to store the node nums in subgraph\n",
    "        for i in range(len(batch)):\n",
    "            if batch[i] != comp:\n",
    "                graph_pos_in_batch.append(i)\n",
    "                graph_length.append(i-pos)\n",
    "                comp = batch[i]\n",
    "                pos = i\n",
    "                graph_length.append(len(batch)-pos)\n",
    "        return graph_pos_in_batch, graph_length        \n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        edge_index_list, edge_attr_list = self.partition_graph(x, edge_index, edge_attr, batch)\n",
    "        x = self.embed(x)\n",
    "        x = x.squeeze(1)\n",
    "        subgraph_pool_list = [\n",
    "            self.subgraph_forward(x, edge_index_list[i].to(self.device), edge_attr_list[i].to(self.device), batch)\n",
    "            for i in range(len(edge_index_list))\n",
    "        ]\n",
    "        graph_pool = self.subgraph_forward(x, edge_index, edge_attr, batch)\n",
    "        subgraph_pool_seq = torch.stack(subgraph_pool_list)\n",
    "        h0 = torch.zeros(self.lstm_layers_num, subgraph_pool_seq.size(1) ,self.lstm_hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.lstm_layers_num, subgraph_pool_seq.size(1) ,self.lstm_hidden_size).to(self.device)\n",
    "        subgraph_output, (_, _) = self.lstm(subgraph_pool_seq, (h0, c0))\n",
    "        return self.fc(torch.cat((subgraph_output[-1], graph_pool), dim=1))\n",
    "        \n",
    "            "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, rnn_dropout, device):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, dropout=rnn_dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        output, _ = self.gru(input, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, vocab_len, graph_embedding_size, gnn_layers_num, lstm_layers_num, lstm_hidden_size, divide_node_num, \n",
    "    decoder_input_size, decoder_hidden_size, decoder_output_size, decoder_num_layers, decoder_rnn_dropout, max_seq_len, device):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = SequenceGNNEncoder(vocab_len, graph_embedding_size, gnn_layers_num, lstm_layers_num, lstm_hidden_size, divide_node_num, \n",
    "    decoder_input_size, device)\n",
    "        self.decoder = RNNDecoder(decoder_input_size, decoder_hidden_size, decoder_output_size, decoder_num_layers, decoder_rnn_dropout, device)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.decoder_num_layers = decoder_num_layers\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        decoder_input = self.encoder(x, edge_index, edge_attr, batch)\n",
    "        # print('decoder_input', decoder_input)\n",
    "        decoder_input = decoder_input.unsqueeze(0)\n",
    "        decoder_input = decoder_input.expand(self.max_seq_len, -1, -1)\n",
    "        decoder_h0 = torch.zeros(self.decoder_num_layers, decoder_input.size(1), self.decoder_hidden_size).to(device)\n",
    "        return self.decoder(decoder_input, decoder_h0)             \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "model_args = {\n",
    "    'vocab_len': vocab_astnodes_size,\n",
    "    'graph_embedding_size': graph_embedding_size,\n",
    "    'gnn_layers_num': gnn_layers_num,\n",
    "    'lstm_layers_num': lstm_layers_num,\n",
    "    'lstm_hidden_size': lstm_hidden_size,\n",
    "    'divide_node_num': divide_node_num,\n",
    "    'decoder_input_size': decoder_input_size,\n",
    "    'decoder_hidden_size': decoder_hidden_size,\n",
    "    'decoder_output_size': vocab_nl_size,\n",
    "    'decoder_num_layers': decoder_num_layers,\n",
    "    'decoder_rnn_dropout': decoder_rnn_dropout,\n",
    "    'max_seq_len': max_seq_len,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "model = EncoderDecoder(**model_args).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda= lambda epoch: decay_ratio ** epoch)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_summary = summary(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mce = nn.CrossEntropyLoss(size_average=False, ignore_index=PAD)\n",
    "def masked_cross_entropy(logits, target):\n",
    "    return mce(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "metric = Bleu(ngram=4, smooth='smooth1')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compute_loss(data, model, optimizer=None, is_train=True):\n",
    "    x = (data.x).to(device)\n",
    "    edge_index = (data.edge_index).to(device)\n",
    "    edge_attr = (data.edge_attr).to(device)\n",
    "    batch = (data.batch).to(device)\n",
    "    y = (data.y).to(device)\n",
    "    pred_y = model(x, edge_index, edge_attr, batch)\n",
    "    y = torch.stack(torch.split(y, max_seq_len))\n",
    "    loss = masked_cross_entropy(pred_y.contiguous(), y.contiguous())\n",
    "\n",
    "    if is_train:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    y = y.contiguous().data.cpu().tolist()\n",
    "    pred = pred_y.max(dim=-1)[1].data.cpu().numpy().T.tolist()\n",
    "\n",
    "    return loss.item(), y, pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compute_bleu4(metric, refs, hyps):\n",
    "    metric.reset()\n",
    "    for i in range(len(refs)):\n",
    "        metric.update((hyps[i], [refs[i]]))\n",
    "    return metric.compute()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, filename = None, patience=3, warm_up=0, verbose=False):\n",
    "\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.warm_up = warm_up\n",
    "        self.filename = filename\n",
    "\n",
    "    def __call__(self, score, model, epoch):\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(score, model)\n",
    "            \n",
    "        elif (score <= self.best_score) and (epoch > self.warm_up) :\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            if (epoch <= self.warm_up):\n",
    "                print('Warming up until epoch', self.warm_up)\n",
    "            \n",
    "            else:\n",
    "                if self.verbose:\n",
    "                    print(f'Score improved. ({self.best_score:.6f} --> {score:.6f}).')\n",
    "                \n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(score, model)\n",
    "                self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, score, model):\n",
    "        \n",
    "        if self.filename is not None:\n",
    "            torch.save(model.state_dict(), self.filename)\n",
    "            \n",
    "        if self.verbose:\n",
    "            print('Model saved...')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fname = exp_dir + save_name\n",
    "early_stopping = EarlyStopping(fname, patience, warm_up, verbose=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for epoch in range(1, num_epoches + 1):\n",
    "    train_loss = 0.\n",
    "    train_refs = []\n",
    "    train_hyps = []\n",
    "    valid_loss = 0.\n",
    "    valid_refs = []\n",
    "    valid_hyps = []\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    for data in tqdm(train_loader, total=len(train_loader), desc='TRAIN'):\n",
    "        loss, gold, pred = compute_loss(data, model, optimizer, is_train=True)\n",
    "        train_loss += loss\n",
    "        train_refs += gold\n",
    "        train_hyps += pred\n",
    "    \n",
    "\n",
    "    # valid\n",
    "    model.eval()\n",
    "    for data in tqdm(valid_loader, total=len(valid_loader), desc='VALID'):\n",
    "        loss, gold, pred = compute_loss(data, model, optimizer, is_train=False)\n",
    "        valid_loss += loss\n",
    "        valid_refs += gold\n",
    "        valid_hyps += pred\n",
    "    \n",
    "    \n",
    "    train_loss = np.sum(train_loss) / len(train_data)\n",
    "    valid_loss = np.sum(valid_loss) / len(valid_data)\n",
    "    train_bleu4 = compute_bleu4(metric, train_refs, train_hyps)\n",
    "    valid_bleu4 = compute_bleu4(metric, valid_refs, valid_hyps)    \n",
    "\n",
    "    msgr.print_msg('Epoch {}: train_loss: {:5.2f}  train_bleu4: {:2.4f}  valid_loss: {:5.2f}  valid_bleu4: {:2.4f}'.format(\n",
    "            epoch, train_loss, train_bleu4, valid_loss, valid_bleu4))\n",
    "    \n",
    "    early_stopping(valid_bleu4, model, epoch)\n",
    "    if early_stopping.early_stop:\n",
    "        msgr.print_msg(\"Early stopping\")\n",
    "        break\n",
    "    \n",
    "    print('-'*80)\n",
    "    scheduler.step()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = EncoderDecoder(**model_args).to(device)\n",
    "fname = exp_dir + save_name\n",
    "ckpt = torch.load(fname)\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()\n",
    "\n",
    "test_refs = []\n",
    "test_hyps = []\n",
    "\n",
    "for data in tqdm(test_loader, total=len(test_loader), desc='TEST'):\n",
    "    x = (data.x).to(device)\n",
    "    edge_index = (data.edge_index).to(device)\n",
    "    edge_attr = (data.edge_attr).to(device)\n",
    "    batch = (data.batch).to(device)\n",
    "    y = (data.y).to(device)\n",
    "    pred_y = model(x, edge_index, edge_attr, batch)\n",
    "    y = torch.stack(torch.split(y, max_seq_len))\n",
    "    pred = pred_y.max(dim=-1)[1].data.cpu().numpy().T.tolist()\n",
    "    test_refs += y\n",
    "    test_hyps += pred\n",
    "\n",
    "test_bleu4 = compute_bleu4(metric, test_refs, test_hyps)\n",
    "msgr.print_msg('test_bleu4: {:2.4f}'.format(test_bleu4))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}