{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import javalang\n",
    "from javalang.ast import Node\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Process, cpu_count, Manager, Pool \n",
    "import os\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, DataCollatorWithPadding\n",
    "from anytree import AnyNode\n",
    "import json\n",
    "from torch_geometric.data import Data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "divide_node_num = 30\n",
    "MAX_NODE_NUM = 300 # the max num of subgraph, set for zero padding \n",
    "max_subgraph_num = int(MAX_NODE_NUM/divide_node_num) \n",
    "max_source_length = 256\n",
    "max_target_length = 32"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_url = '/data/code/represent-code-in-human/data/TLC-SUM/train.json'\n",
    "valid_url = '/data/code/represent-code-in-human/data/TLC-SUM/valid.json'\n",
    "test_url = '/data/code/represent-code-in-human/data/TLC-SUM/test.json'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data = pd.read_json(path_or_buf=train_url, lines=True)\n",
    "valid_data = pd.read_json(path_or_buf=valid_url, lines=True)\n",
    "test_data = pd.read_json(path_or_buf=test_url, lines=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data['docstring_tokens'] = train_data['comment'].str.split()\n",
    "valid_data['docstring_tokens'] = valid_data['comment'].str.split()\n",
    "test_data['docstring_tokens'] = test_data['comment'].str.split()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(valid_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(test_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Delete the codes that cannot be parsed by javalang"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def parse_program(func):\n",
    "    tokens = javalang.tokenizer.tokenize(func)\n",
    "    parser = javalang.parser.Parser(tokens)\n",
    "    tree = parser.parse_member_declaration()\n",
    "    return tree"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_syntax_error_ids(data):\n",
    "    syntax_error_ids = []\n",
    "    for i in tqdm(range(len(data['code']))):\n",
    "        try:\n",
    "            tree = parse_program(data['code'][i])\n",
    "        except:\n",
    "            syntax_error_ids.append(i)\n",
    "    return syntax_error_ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_syntax_error_ids = get_syntax_error_ids(train_data)\n",
    "len(train_syntax_error_ids)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "valid_syntax_error_ids = get_syntax_error_ids(valid_data)\n",
    "len(valid_syntax_error_ids)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_syntax_error_ids = get_syntax_error_ids(test_data)\n",
    "len(test_syntax_error_ids)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data_new = train_data.drop(train_syntax_error_ids)\n",
    "valid_data_new = valid_data.drop(valid_syntax_error_ids)\n",
    "test_data_new = test_data.drop(test_syntax_error_ids)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data_new = train_data_new.sample(frac=1).reset_index(drop=True)\n",
    "valid_data_new = valid_data_new.sample(frac=1).reset_index(drop=True)\n",
    "test_data_new = test_data_new.sample(frac=1).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data_new"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "enhance code by ast and description"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "java_api_url = '/data/code/represent-code-in-human/data/java_api.csv'\n",
    "java_api = pd.read_csv(java_api_url, header=0, encoding='utf-8')\n",
    "java_api['index_name'] = java_api['index_name'].apply(str)\n",
    "java_api "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_token(node):\n",
    "    token = ''\n",
    "    if isinstance(node, str):\n",
    "        token = node\n",
    "    elif isinstance(node, set):\n",
    "        token = 'Modifier'\n",
    "    elif isinstance(node, Node):\n",
    "        token = node.__class__.__name__\n",
    "    return token\n",
    "\n",
    "\n",
    "def get_child(root):\n",
    "    if isinstance(root, Node):\n",
    "        children = root.children\n",
    "    elif isinstance(root, set):\n",
    "        children = list(root)\n",
    "    else:\n",
    "        children = []\n",
    "\n",
    "    def expand(nested_list):\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, list):\n",
    "                yield from expand(item)\n",
    "            elif item:\n",
    "                yield item\n",
    "\n",
    "    return list(expand(children))\n",
    "\n",
    "\n",
    "def get_sequence(node, sequence, api_sequence):\n",
    "    token, children = get_token(node), get_child(node)\n",
    "    sequence.append(token)\n",
    "    if token == 'MethodInvocation':\n",
    "        api = [get_token(child) for child in children if not get_child(child)]\n",
    "        # api_sequence.append(' '.join(api))\n",
    "        if len(api) > 1:\n",
    "            api_sequence.append(api[-1])\n",
    "    for child in children:\n",
    "        get_sequence(child, sequence, api_sequence)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def api_match(api_sequence, java_api):\n",
    "    description_sequence = []\n",
    "    for api in api_sequence:\n",
    "        loc = java_api.loc[java_api['index_name'].str.contains(api, case=True)]\n",
    "        if not loc.empty:\n",
    "            description = loc['method_description'].iloc[0]\n",
    "            if description != 'None':\n",
    "                description_sequence.append(description)\n",
    "    return description_sequence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_ast_and_description(data):\n",
    "    description_sequence = []\n",
    "    ast_sequence = []\n",
    "    ast_sum = 0\n",
    "    description_sum = 0\n",
    "    data_size = len(data)\n",
    "    for i in tqdm(range(data_size)):\n",
    "        sequence = []\n",
    "        api_sequence = []    \n",
    "        get_sequence(parse_program(data['code'].iloc[i]), sequence, api_sequence)\n",
    "        ast = ' '.join(sequence)\n",
    "        ast_sequence.append(ast) \n",
    "        ast_sum += len(ast.split(' '))\n",
    "\n",
    "        api_sequence = list(set(api_sequence)) \n",
    "        description = ' '.join(api_match(api_sequence, java_api)) \n",
    "        description_sequence.append(description) \n",
    "        description_sum += len(description.split(' '))\n",
    "    print('ast average length', ast_sum/data_size)\n",
    "    print('description average length', description_sum/data_size)\n",
    "    return description_sequence, ast_sequence   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# multi-process\n",
    "data_new = train_data_new\n",
    "def multi_get_ast_and_des(l, i):\n",
    "    sequence = []\n",
    "    api_sequence = []    \n",
    "    get_sequence(parse_program(data_new['code'].iloc[i]), sequence, api_sequence)\n",
    "    ast = ' '.join(sequence)\n",
    "    api_sequence = list(set(api_sequence)) \n",
    "    des = ' '.join(api_match(api_sequence, java_api)) \n",
    "    d = {'ast': ast, 'des': des, 'i': i}\n",
    "    l.append(d)\n",
    "\n",
    "\n",
    "manager = Manager()\n",
    "data_size = len(data_new)\n",
    "# print('data_size', data_size)\n",
    "l = manager.list()\n",
    "p = Pool(processes=30)\n",
    "for i in range(data_size):\n",
    "    p.apply_async(multi_get_ast_and_des, (l, i))\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "ast = []\n",
    "des = []\n",
    "i = []\n",
    "for d in l[:]:\n",
    "    ast.append(d['ast'].encode('utf-8','ignore').decode(\"utf-8\"))\n",
    "    des.append(d['des'].encode('utf-8','ignore').decode(\"utf-8\"))\n",
    "    i.append(d['i'])\n",
    "d = {'ast': ast, 'des': des, 'i': i}\n",
    "train_df = pd.DataFrame.from_dict(d)    \n",
    "train_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# multi-process\n",
    "data_new = test_data_new\n",
    "def multi_get_ast_and_des(l, i):\n",
    "    sequence = []\n",
    "    api_sequence = []    \n",
    "    get_sequence(parse_program(data_new['code'].iloc[i]), sequence, api_sequence)\n",
    "    ast = ' '.join(sequence)\n",
    "    api_sequence = list(set(api_sequence)) \n",
    "    des = ' '.join(api_match(api_sequence, java_api)) \n",
    "    d = {'ast': ast, 'des': des, 'i': i}\n",
    "    l.append(d)\n",
    "\n",
    "\n",
    "manager = Manager()\n",
    "data_size = len(data_new)\n",
    "# print('data_size', data_size)\n",
    "l = manager.list()\n",
    "p = Pool(processes=30)\n",
    "for i in range(data_size):\n",
    "    p.apply_async(multi_get_ast_and_des, (l, i))\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "ast = []\n",
    "des = []\n",
    "i = []\n",
    "for d in l[:]:\n",
    "    ast.append(d['ast'].encode('utf-8','ignore').decode(\"utf-8\"))\n",
    "    des.append(d['des'].encode('utf-8','ignore').decode(\"utf-8\"))\n",
    "    i.append(d['i'])\n",
    "d = {'ast': ast, 'des': des, 'i': i}\n",
    "test_df = pd.DataFrame.from_dict(d)    \n",
    "test_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# multi-process\n",
    "data_new = valid_data_new\n",
    "def multi_get_ast_and_des(l, i):\n",
    "    sequence = []\n",
    "    api_sequence = []    \n",
    "    get_sequence(parse_program(data_new['code'].iloc[i]), sequence, api_sequence)\n",
    "    ast = ' '.join(sequence)\n",
    "    api_sequence = list(set(api_sequence)) \n",
    "    des = ' '.join(api_match(api_sequence, java_api)) \n",
    "    d = {'ast': ast, 'des': des, 'i': i}\n",
    "    l.append(d)\n",
    "\n",
    "\n",
    "manager = Manager()\n",
    "data_size = len(data_new)\n",
    "# print('data_size', data_size)\n",
    "l = manager.list()\n",
    "p = Pool(processes=30)\n",
    "for i in range(data_size):\n",
    "    p.apply_async(multi_get_ast_and_des, (l, i))\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "ast = []\n",
    "des = []\n",
    "i = []\n",
    "for d in l[:]:\n",
    "    ast.append(d['ast'].encode('utf-8','ignore').decode(\"utf-8\"))\n",
    "    des.append(d['des'].encode('utf-8','ignore').decode(\"utf-8\"))\n",
    "    i.append(d['i'])\n",
    "d = {'ast': ast, 'des': des , 'i': i}\n",
    "valid_df = pd.DataFrame.from_dict(d)    \n",
    "valid_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df = train_df.sort_values(by=['i']).reset_index(drop=True)\n",
    "train_data_new['ast'] = train_df['ast'].to_list()\n",
    "train_data_new['des'] = train_df['des'].to_list()\n",
    "train_data_new['ast_des'] = train_data_new['ast'] + ' ' + train_data_new['des']\n",
    "train_data_new"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "valid_df = valid_df.sort_values(by=['i']).reset_index(drop=True)\n",
    "valid_data_new['ast'] = valid_df['ast'].to_list()\n",
    "valid_data_new['des'] = valid_df['des'].to_list()\n",
    "valid_data_new['ast_des'] = valid_data_new['ast'] + ' ' + valid_data_new['des']\n",
    "valid_data_new"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_df = test_df.sort_values(by=['i']).reset_index(drop=True)\n",
    "test_data_new['ast'] = test_df['ast'].to_list()\n",
    "test_data_new['des'] = test_df['des'].to_list()\n",
    "test_data_new['ast_des'] = test_data_new['ast'] + ' ' + test_data_new['des']\n",
    "test_data_new"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "valid_data_new.to_json(path_or_buf='/data/code/represent-code-in-human/data/TLC-SUM-enhanced/valid.jsonl',\n",
    "                     orient='records', lines=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_data_new.to_json(path_or_buf='/data/code/represent-code-in-human/data/TLC-SUM-enhanced/test.jsonl',\n",
    "                     orient='records', lines=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data_new.to_json(path_or_buf='/data/code/represent-code-in-human/data/TLC-SUM-enhanced/train.jsonl',\n",
    "                     orient='records', lines=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "statistics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def statistics(data):\n",
    "    ast_length = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        ast_length.append(len(data['ast'][i].split()))\n",
    "    series = pd.Series(ast_length)\n",
    "    print(series.describe())   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "statistics(train_data_new)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "statistics(valid_data_new)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "statistics(test_data_new)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "write features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# use javalang to generate ASTs and depth-first traverse to generate ast nodes corpus\n",
    "def get_token(node):\n",
    "    token = 'None'\n",
    "    if isinstance(node, str):\n",
    "        token = node\n",
    "    elif isinstance(node, set):\n",
    "        token = 'Modifier'\n",
    "    elif isinstance(node, Node):\n",
    "        token = node.__class__.__name__\n",
    "    return token\n",
    "\n",
    "\n",
    "def get_child(root):\n",
    "    if isinstance(root, Node):\n",
    "        children = root.children\n",
    "    elif isinstance(root, set):\n",
    "        children = list(root)\n",
    "    else:\n",
    "        children = []\n",
    "\n",
    "    def expand(nested_list):\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, list):\n",
    "                for sub_item in expand(item):\n",
    "                    yield sub_item\n",
    "            elif item:\n",
    "                yield item\n",
    "\n",
    "    return list(expand(children))\n",
    "\n",
    "\n",
    "def get_sequence(node, sequence):\n",
    "    token, children = get_token(node), get_child(node)\n",
    "    sequence.append(token)\n",
    "    for child in children:\n",
    "        get_sequence(child, sequence)\n",
    "\n",
    "\n",
    "def parse_program(func):\n",
    "    tokens = javalang.tokenizer.tokenize(func)\n",
    "    parser = javalang.parser.Parser(tokens)\n",
    "    tree = parser.parse_member_declaration()\n",
    "    return tree"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "checkpoint = 'microsoft/codebert-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(checkpoint)\n",
    "ast_tokenizer = RobertaTokenizer.from_pretrained(checkpoint)\n",
    "roberta = RobertaModel.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "config = RobertaConfig.from_pretrained(checkpoint)\n",
    "javalang_special_tokens = ['CompilationUnit','Import','Documented','Declaration','TypeDeclaration','PackageDeclaration',\n",
    "                            'ClassDeclaration','EnumDeclaration','InterfaceDeclaration','AnnotationDeclaration','Type',\n",
    "                            'BasicType','ReferenceType','TypeArgument','TypeParameter','Annotation','ElementValuePair',\n",
    "                            'ElementArrayValue','Member','MethodDeclaration','FieldDeclaration','ConstructorDeclaration',\n",
    "                            'ConstantDeclaration','ArrayInitializer','VariableDeclaration','LocalVariableDeclaration',\n",
    "                            'VariableDeclarator','FormalParameter','InferredFormalParameter','Statement','IfStatement',\n",
    "                            'WhileStatement','DoStatement','ForStatement','AssertStatement','BreakStatement','ContinueStatement',\n",
    "                            'ReturnStatement','ThrowStatement','SynchronizedStatement','TryStatement','SwitchStatement',\n",
    "                            'BlockStatement','StatementExpression','TryResource','CatchClause','CatchClauseParameter',\n",
    "                            'SwitchStatementCase','ForControl','EnhancedForControl','Expression','Assignment','TernaryExpression',\n",
    "                            'BinaryOperation','Cast','MethodReference','LambdaExpression','Primary','Literal','This',\n",
    "                            'MemberReference','Invocation','ExplicitConstructorInvocation','SuperConstructorInvocation',\n",
    "                            'MethodInvocation','SuperMethodInvocation','SuperMemberReference','ArraySelector','ClassReference',\n",
    "                            'VoidClassReference','Creator','ArrayCreator','ClassCreator','InnerClassCreator','EnumBody',\n",
    "                            'EnumConstantDeclaration','AnnotationMethod', 'Modifier']\n",
    "special_tokens_dict = {'additional_special_tokens': javalang_special_tokens}\n",
    "num_added_toks = ast_tokenizer.add_special_tokens(special_tokens_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  generate tree for AST Node\n",
    "def create_tree(root, node, node_list, sub_id_list, leave_list, tokenizer, parent=None):\n",
    "    id = len(node_list)\n",
    "    node_list.append(node)\n",
    "    token, children = get_token(node), get_child(node)\n",
    "\n",
    "    if children == []:\n",
    "        # print('this is a leaf:', token, id)\n",
    "        leave_list.append(id)\n",
    "\n",
    "    # Use roberta.tokenizer to generate subtokens\n",
    "    # If a token can be divided into multiple(>1) subtokens, the first subtoken will be set as the previous node, \n",
    "    # and the other subtokens will be set as its new children\n",
    "    token = token.encode('utf-8','ignore').decode(\"utf-8\")   \n",
    "    sub_token_list = tokenizer.tokenize(token)\n",
    "    \n",
    "    if id == 0:\n",
    "        root.token = sub_token_list[0] # the root node is one of the tokenizer's special tokens\n",
    "        root.data = node\n",
    "        # record the num of nodes for every children of root\n",
    "        root_children_node_num = []\n",
    "        for child in children:\n",
    "            node_num = len(node_list)\n",
    "            create_tree(root, child, node_list, sub_id_list, leave_list, tokenizer, parent=root)\n",
    "            root_children_node_num.append(len(node_list) - node_num)        \n",
    "        return root_children_node_num\n",
    "    else:\n",
    "        # print(sub_token_list)\n",
    "        new_node = AnyNode(id=id, token=sub_token_list[0], data=node, parent=parent)\n",
    "        if len(sub_token_list) > 1:\n",
    "            sub_id_list.append(id)\n",
    "            for sub_token in sub_token_list[1:]:\n",
    "                id += 1\n",
    "                AnyNode(id=id, token=sub_token, data=node, parent=new_node)\n",
    "                node_list.append(sub_token)\n",
    "                sub_id_list.append(id)\n",
    "        \n",
    "        for child in children:\n",
    "            create_tree(root, child, node_list, sub_id_list, leave_list, tokenizer, parent=new_node)\n",
    "    # print(token, id)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# traverse the AST tree to get all the nodes and edges\n",
    "def get_node_and_edge(node, node_index_list, tokenizer, src, tgt, variable_token_list, variable_id_list):\n",
    "    token = node.token\n",
    "    node_index_list.append(tokenizer.convert_tokens_to_ids(token))\n",
    "    # node_index_list.append([vocab_dict.word2id.get(token, UNK)])\n",
    "    # find out all variables\n",
    "    if token in ['VariableDeclarator', 'MemberReference']:\n",
    "        if node.children: # some chidren are comprised by non-utf8 and will be removed\n",
    "            variable_token_list.append(node.children[0].token)\n",
    "            variable_id_list.append(node.children[0].id)   \n",
    "    \n",
    "    for child in node.children:\n",
    "        src.append(node.id)\n",
    "        tgt.append(child.id)\n",
    "        src.append(child.id)\n",
    "        tgt.append(node.id)\n",
    "        get_node_and_edge(child, node_index_list, tokenizer, src, tgt, variable_token_list, variable_id_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# generate pytorch_geometric input format data from ast\n",
    "def get_pyg_data_from_ast(ast, tokenizer):\n",
    "    node_list = []\n",
    "    sub_id_list = [] # record the ids of node that can be divide into multple subtokens\n",
    "    leave_list = [] # record the ids of leave \n",
    "    new_tree = AnyNode(id=0, token=None, data=None)\n",
    "    root_children_node_num = create_tree(new_tree, ast, node_list, sub_id_list, leave_list, tokenizer)\n",
    "    # print('root_children_node_num', root_children_node_num)\n",
    "    x = []\n",
    "    edge_src = []\n",
    "    edge_tgt = []\n",
    "    # record variable tokens and ids to add data flow edge in AST graph\n",
    "    variable_token_list = []\n",
    "    variable_id_list = []\n",
    "    get_node_and_edge(new_tree, x, tokenizer, edge_src, edge_tgt, variable_token_list, variable_id_list)\n",
    "\n",
    "    ast_edge_num = len(edge_src)\n",
    "    edge_attr = [[0] for _ in range(ast_edge_num)]\n",
    "    # set subtoken edge type to 2\n",
    "    for i in range(len(edge_attr)):\n",
    "        if edge_src[i] in sub_id_list and edge_tgt[i] in sub_id_list:\n",
    "            edge_attr[i] = [2]\n",
    "    # add data flow edge\n",
    "    variable_dict = {}\n",
    "    for i in range(len(variable_token_list)):\n",
    "        # print('variable_dict', variable_dict)\n",
    "        if variable_token_list[i] not in variable_dict:\n",
    "            variable_dict.setdefault(variable_token_list[i], variable_id_list[i])\n",
    "        else:\n",
    "            # print('edge', variable_dict.get(variable_token_list[i]), variable_id_list[i])\n",
    "            edge_src.append(variable_dict.get(variable_token_list[i]))\n",
    "            edge_tgt.append(variable_id_list[i])\n",
    "            edge_src.append(variable_id_list[i])\n",
    "            edge_tgt.append(variable_dict.get(variable_token_list[i]))\n",
    "            variable_dict[variable_token_list[i]] = variable_id_list[i]\n",
    "    dataflow_edge_num = len(edge_src) - ast_edge_num\n",
    "\n",
    "    # add next-token edge\n",
    "    nexttoken_edge_num = len(leave_list)-1\n",
    "    for i in range(nexttoken_edge_num):\n",
    "        edge_src.append(leave_list[i])\n",
    "        edge_tgt.append(leave_list[i+1])\n",
    "        edge_src.append(leave_list[i+1])\n",
    "        edge_tgt.append(leave_list[i])\n",
    "\n",
    "    edge_index = [edge_src, edge_tgt]\n",
    "\n",
    "    # set data flow edge type to 1\n",
    "    for _ in range(dataflow_edge_num):\n",
    "        edge_attr.append([1])\n",
    "    \n",
    "    # set data flow edge type to 3\n",
    "    for _ in range(nexttoken_edge_num * 2):\n",
    "        edge_attr.append([3])\n",
    "    \n",
    "    return x, edge_index, edge_attr, root_children_node_num"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def get_subgraph_node_num(root_children_node_num, divide_node_num):\n",
    "    subgraph_node_num = []\n",
    "    node_sum = 0\n",
    "    real_graph_num = 0\n",
    "    for num in root_children_node_num:\n",
    "        node_sum += num\n",
    "        if node_sum >= divide_node_num:\n",
    "            subgraph_node_num.append(node_sum)\n",
    "            node_sum = 0    \n",
    "    \n",
    "    subgraph_node_num.append(node_sum)\n",
    "    real_graph_num = len(subgraph_node_num)\n",
    "\n",
    "    if real_graph_num >= max_subgraph_num:\n",
    "        return subgraph_node_num[: max_subgraph_num], max_subgraph_num\n",
    "\n",
    "    # print(len(subgraph_node_num))\n",
    "    # if the last subgraph node num < divide_node_num, then put the last subgraph to the second to last subgraph\n",
    "    # if subgraph_node_num[-1] < divide_node_num:\n",
    "    #     subgraph_node_num[-2] = subgraph_node_num[-2] + subgraph_node_num[-1]\n",
    "    #     subgraph_node_num[-1] = 0\n",
    "    #     real_graph_num -= 1\n",
    "\n",
    "    # zero padding for tensor transforming\n",
    "    for _ in range(real_graph_num, max_subgraph_num):\n",
    "        subgraph_node_num.append(0)\n",
    "    \n",
    "    return subgraph_node_num, real_graph_num"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def convert_examples_to_features(examples, ast_tokenizer, tokenizer, stage=None):\n",
    "    features = []\n",
    "    for example in tqdm(examples):\n",
    "        # pyg\n",
    "        ast = parse_program(example.source)\n",
    "        x, edge_index, edge_attr, root_children_node_num = get_pyg_data_from_ast(ast, ast_tokenizer)\n",
    "        subgraph_node_num, real_graph_num = get_subgraph_node_num(root_children_node_num, divide_node_num)\n",
    "\n",
    "        # source\n",
    "        source_tokens = tokenizer.tokenize(example.ast_des)[: max_source_length-2]\n",
    "        source_tokens = [tokenizer.cls_token] + source_tokens + [tokenizer.sep_token]\n",
    "        source_ids = tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "        source_mask = [1] * (len(source_ids))\n",
    "        padding_length = max_source_length - len(source_ids)\n",
    "        source_ids += [tokenizer.pad_token_id] * padding_length\n",
    "        source_mask += [0] * padding_length\n",
    "\n",
    "        # target\n",
    "        if stage == 'test':\n",
    "            target_tokens = tokenizer.tokenize('None')\n",
    "        else:\n",
    "            target_tokens = tokenizer.tokenize(example.target)[: max_target_length-2]\n",
    "        target_tokens = [tokenizer.cls_token] + target_tokens + [tokenizer.sep_token]\n",
    "        target_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "        target_mask = [1] * len(target_ids)\n",
    "        padding_length = max_target_length - len(target_ids)\n",
    "        target_ids += [tokenizer.pad_token_id] * padding_length\n",
    "        target_mask += [0] * padding_length\n",
    "\n",
    "        features.append(\n",
    "            Data(\n",
    "                x= torch.tensor(x, dtype=torch.long),\n",
    "                edge_index=torch.tensor(edge_index, dtype=torch.long),\n",
    "                edge_attr=torch.tensor(edge_attr, dtype=torch.long),\n",
    "                source_ids=torch.tensor(source_ids, dtype=torch.long),\n",
    "                source_mask=torch.tensor(source_mask, dtype=torch.long),\n",
    "                target_ids=torch.tensor(target_ids, dtype=torch.long),\n",
    "                target_mask=torch.tensor(target_mask, dtype=torch.long),\n",
    "                subgraph_node_num=torch.tensor(subgraph_node_num, dtype=torch.long),\n",
    "                real_graph_num=torch.tensor(real_graph_num, dtype=torch.long)\n",
    "            )\n",
    "        )\n",
    "    return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Example(object):\n",
    "    def __init__(self, idx, source, ast_des, target):\n",
    "        self.idx = idx\n",
    "        self.source = source\n",
    "        self.ast_des = ast_des\n",
    "        self.target = target"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# read dataset\n",
    "def read_examples(filename):\n",
    "    examples = []\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            js = json.loads(line)\n",
    "            if 'idx' not in js:\n",
    "                js['idx'] = idx\n",
    "            \n",
    "            code = js['code']\n",
    "            nl = ' '.join(js['docstring_tokens']).replace('\\n', '')\n",
    "            nl = ' '.join(nl.strip().split())\n",
    "            ast_des = js['ast_des']\n",
    "            examples.append(\n",
    "                Example(\n",
    "                    idx = idx,\n",
    "                    source = code,\n",
    "                    ast_des = ast_des,\n",
    "                    target = nl,\n",
    "                )\n",
    "            )\n",
    "    return examples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_examples = read_examples('/data/code/represent-code-in-human/data/TLC-SUM-enhanced/train.jsonl')\n",
    "valid_examples = read_examples('/data/code/represent-code-in-human/data/TLC-SUM-enhanced/valid.jsonl')\n",
    "test_examples = read_examples('/data/code/represent-code-in-human/data/TLC-SUM-enhanced/test.jsonl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_x = []\n",
    "valid_x = []\n",
    "test_x = []\n",
    "for example in train_examples:\n",
    "    ast = parse_program(example.source)\n",
    "    x, edge_index, edge_attr, root_children_node_num = get_pyg_data_from_ast(ast, ast_tokenizer)\n",
    "    train_x.append(len(x))\n",
    "\n",
    "for example in valid_examples:\n",
    "    ast = parse_program(example.source)\n",
    "    x, edge_index, edge_attr, root_children_node_num = get_pyg_data_from_ast(ast, ast_tokenizer)\n",
    "    valid_x.append(len(x))\n",
    "\n",
    "for example in test_examples:\n",
    "    ast = parse_program(example.source)\n",
    "    x, edge_index, edge_attr, root_children_node_num = get_pyg_data_from_ast(ast, ast_tokenizer)\n",
    "    test_x.append(len(x))\n",
    "\n",
    "all_x = train_x + valid_x + test_x\n",
    "\n",
    "print(sum(train_x)/len(train_x), sum(valid_x)/len(valid_x), sum(test_x)/len(test_x), sum(all_x)/len(all_x))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_features = convert_examples_to_features(train_examples, ast_tokenizer, tokenizer, stage='train')\n",
    "valid_features = convert_examples_to_features(valid_examples, ast_tokenizer, tokenizer, stage='valid')\n",
    "test_features = convert_examples_to_features(test_examples, ast_tokenizer, tokenizer, stage='test')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# torch.save(train_features,'features/tlc/train_features.pt')\n",
    "# torch.save(valid_features,'features/tlc/valid_features.pt')\n",
    "# torch.save(test_features,'features/tlc/test_features.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(train_features)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(valid_features)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(test_features)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}