{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, DataCollatorWithPadding, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset, SequentialSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from codebert_seq2seq import Seq2Seq\n",
    "from torchsummary import summary\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import cycle\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "import bleu\n",
    "import numpy as np\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Info(object):\n",
    "    def __init__(self, info_prefix=''):\n",
    "        self.info_prefix = info_prefix\n",
    "    \n",
    "    def print_msg(self, msg):\n",
    "        text = self.info_prefix + ' ' + msg\n",
    "        print(text)\n",
    "        logging.info(text)\n",
    "\n",
    "\n",
    "run_id = datetime.now().strftime('%Y-%m-%d--%H-%M-%S')\n",
    "log_file = 'logs/' + run_id + '.log'\n",
    "exp_dir = 'runs/' + run_id\n",
    "os.mkdir(exp_dir)\n",
    "logging.basicConfig(format='%(asctime)s | %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', filename=log_file, level=logging.DEBUG)\n",
    "msgr = Info('codebert enhanced')\n",
    "\n",
    "msgr.print_msg('run_id : {}'.format(run_id))\n",
    "msgr.print_msg('log_file : {}'.format(log_file))\n",
    "msgr.print_msg('exp_dir: {}'.format(exp_dir))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "checkpoint = 'microsoft/codebert-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(checkpoint)\n",
    "roberta = RobertaModel.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "config = RobertaConfig.from_pretrained(checkpoint)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_source_length = 256\n",
    "max_target_length = 32\n",
    "batch_size = 8\n",
    "beam_size = 10\n",
    "lr = 5e-5\n",
    "warmup_steps = 0\n",
    "train_steps = 50000\n",
    "weight_decay = 0.0\n",
    "adam_epsilon = 1e-8\n",
    "valid_steps = 1000\n",
    "train_url = '/data/code/represent-code-in-human/data/code-summarization-enhanced-full/train.jsonl'\n",
    "valid_url = '/data/code/represent-code-in-human/data/code-summarization-enhanced-full/valid.jsonl'\n",
    "test_url = '/data/code/represent-code-in-human/data/code-summarization-enhanced-full/test.jsonl'\n",
    "output_dir = exp_dir"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads)\n",
    "decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "model = Seq2Seq(encoder=roberta, decoder=decoder, config=config, beam_size=10, max_length=max_target_length, \n",
    "                sos_id=tokenizer.cls_token_id, eos_id=tokenizer.sep_token_id)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "summary(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device('cuda: 0')\n",
    "model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Example(object):\n",
    "    def __init__(self, idx, source, target):\n",
    "        self.idx = idx\n",
    "        self.source = source\n",
    "        self.target = target"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_examples(filename):\n",
    "    examples = []\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            js = json.loads(line)\n",
    "            if 'idx' not in js:\n",
    "                js['idx'] = idx\n",
    "            ast_des = js['ast_des'].strip()\n",
    "            nl = ' '.join(js['docstring_tokens']).replace('\\n', '')\n",
    "            nl = ' '.join(nl.strip().split())\n",
    "            examples.append(\n",
    "                Example(\n",
    "                    idx = idx,\n",
    "                    source = ast_des,\n",
    "                    target = nl,\n",
    "                )\n",
    "            )\n",
    "    return examples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class InputFeatures(object):\n",
    "    def __init__(self, example_id, source_ids, target_ids, source_mask, target_mask):\n",
    "        self.example_id = example_id\n",
    "        self.source_ids = source_ids\n",
    "        self.target_ids = target_ids\n",
    "        self.source_mask = source_mask\n",
    "        self.target_mask = target_mask"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def convert_examples_to_features(examples, tokenizer, stage=None):\n",
    "    features = []\n",
    "    for example_index, example in enumerate(examples):\n",
    "        # source\n",
    "        source_tokens = tokenizer.tokenize(example.source)[: max_source_length-2]\n",
    "        source_tokens = [tokenizer.cls_token] + source_tokens + [tokenizer.sep_token]\n",
    "        source_ids = tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "        source_mask = [1] * (len(source_ids))\n",
    "        padding_length = max_source_length - len(source_ids)\n",
    "        source_ids += [tokenizer.pad_token_id] * padding_length\n",
    "        source_mask += [0] * padding_length\n",
    "\n",
    "        # target\n",
    "        if stage == 'test':\n",
    "            target_tokens = tokenizer.tokenize('None')\n",
    "        else:\n",
    "            target_tokens = tokenizer.tokenize(example.target)[: max_target_length-2]\n",
    "        target_tokens = [tokenizer.cls_token] + target_tokens + [tokenizer.sep_token]\n",
    "        target_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "        target_mask = [1] * len(target_ids)\n",
    "        padding_length = max_target_length - len(target_ids)\n",
    "        target_ids += [tokenizer.pad_token_id] * padding_length\n",
    "        target_mask += [0] * padding_length\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                example_index,\n",
    "                source_ids,\n",
    "                target_ids,\n",
    "                source_mask,\n",
    "                target_mask\n",
    "            )\n",
    "        )\n",
    "    return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_examples = read_examples(train_url)\n",
    "train_features = convert_examples_to_features(train_examples, tokenizer, stage='train')\n",
    "all_source_ids = torch.tensor([f.source_ids for f in train_features], dtype=torch.long)\n",
    "all_source_mask = torch.tensor([f.source_mask for f in train_features], dtype=torch.long)\n",
    "all_target_ids = torch.tensor([f.target_ids for f in train_features], dtype=torch.long)\n",
    "all_target_mask = torch.tensor([f.target_mask for f in train_features], dtype=torch.long)    \n",
    "train_data = TensorDataset(all_source_ids,all_source_mask, all_target_ids, all_target_mask)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# optimizer and schedule\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=train_steps)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Start training\n",
    "msgr.print_msg(\"***** Running training *****\")\n",
    "msgr.print_msg(\"  Num examples = {}\".format(len(train_examples)))\n",
    "msgr.print_msg(\"  Batch size = {}\".format(batch_size))\n",
    "msgr.print_msg(\"  Num epoch = {}\".format(batch_size//len(train_examples)))\n",
    "model.train()\n",
    "valid_dataset = {}\n",
    "nb_tr_examples, nb_tr_steps, tr_loss, global_step, best_bleu, best_loss = 0, 0, 0, 0, 0, 1e6\n",
    "bar = tqdm(range(train_steps), total=train_steps)\n",
    "train_dataloader = cycle(train_dataloader)\n",
    "\n",
    "for step in bar:\n",
    "    batch = next(train_dataloader)\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    source_ids, source_mask, target_ids, target_mask = batch\n",
    "    print('source_ids', source_ids)\n",
    "    print('target_ids', target_ids)\n",
    "    loss, _, _, = model(source_ids=source_ids, source_mask=source_mask, target_ids=target_ids, target_mask=target_mask)\n",
    "\n",
    "    tr_loss += loss.item()\n",
    "    train_loss = round(tr_loss / (nb_tr_steps + 1), 4) \n",
    "    bar.set_description('loss {}'.format(train_loss))\n",
    "    nb_tr_examples += source_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step()\n",
    "    global_step += 1\n",
    "    \n",
    "    if (global_step + 1) % valid_steps == 0:\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "        if 'valid_loss' in valid_dataset:\n",
    "            valid_examples, valid_data=valid_dataset['valid_loss']\n",
    "        else:\n",
    "            valid_examples = read_examples(valid_url)\n",
    "            valid_features = convert_examples_to_features(valid_examples, tokenizer, stage='valid')\n",
    "            all_source_ids = torch.tensor([f.source_ids for f in valid_features], dtype=torch.long)\n",
    "            all_source_mask = torch.tensor([f.source_mask for f in valid_features], dtype=torch.long)\n",
    "            all_target_ids = torch.tensor([f.target_ids for f in valid_features], dtype=torch.long)\n",
    "            all_target_mask = torch.tensor([f.target_mask for f in valid_features], dtype=torch.long)      \n",
    "            valid_data = TensorDataset(all_source_ids, all_source_mask, all_target_ids, all_target_mask)   \n",
    "            valid_dataset['valid_loss']=valid_examples, valid_data\n",
    "        valid_sampler = SequentialSampler(valid_data)\n",
    "        valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)\n",
    "\n",
    "        msgr.print_msg(\"\\n***** Running evaluation *****\")\n",
    "        msgr.print_msg(\"  Num examples = {}\".format(len(valid_examples)))\n",
    "        msgr.print_msg(\"  Batch size = {}\".format(batch_size))\n",
    "\n",
    "        #Start Evaling model\n",
    "        model.eval()\n",
    "        valid_loss, tokens_num = 0, 0\n",
    "        for batch in valid_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            source_ids, source_mask, target_ids, target_mask = batch                  \n",
    "\n",
    "            with torch.no_grad():\n",
    "                _,loss,num = model(source_ids=source_ids,source_mask=source_mask,\n",
    "                                    target_ids=target_ids,target_mask=target_mask)     \n",
    "            valid_loss += loss.sum().item()\n",
    "            tokens_num += num.sum().item()\n",
    "        #Pring loss of valid dataset    \n",
    "        model.train()\n",
    "        valid_loss = valid_loss / tokens_num\n",
    "        result = {'valid_ppl': round(np.exp(valid_loss), 5),\n",
    "                    'global_step': global_step+1,\n",
    "                    'train_loss': round(train_loss, 5)}\n",
    "        for key in sorted(result.keys()):\n",
    "            msgr.print_msg(\"{}= {}\".format(key, str(result[key])))\n",
    "        msgr.print_msg(\"  \"+\"*\"*20)   \n",
    "        \n",
    "        #save last checkpoint\n",
    "        last_output_dir = os.path.join(output_dir, 'checkpoint-last')\n",
    "        if not os.path.exists(last_output_dir):\n",
    "            os.makedirs(last_output_dir)\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        output_model_file = os.path.join(last_output_dir, \"pytorch_model.bin\")\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)                    \n",
    "        if valid_loss < best_loss:\n",
    "            msgr.print_msg(\"  Best ppl:{}\".format(round(np.exp(valid_loss), 5)))\n",
    "            msgr.print_msg(\"  \" + \"*\" * 20)\n",
    "            best_loss = valid_loss\n",
    "            # Save best checkpoint for best ppl\n",
    "            best_output_dir = os.path.join(output_dir, 'checkpoint-best-ppl')\n",
    "            if not os.path.exists(best_output_dir):\n",
    "                os.makedirs(best_output_dir)\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "            output_model_file = os.path.join(best_output_dir, \"pytorch_model.bin\")\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)  \n",
    "                    \n",
    "                    \n",
    "        #Calculate bleu  \n",
    "        if 'valid_bleu' in valid_dataset:\n",
    "            valid_examples, valid_data = valid_dataset['valid_bleu']\n",
    "        else:\n",
    "            valid_examples = read_examples(valid_url)\n",
    "            valid_examples = random.sample(valid_examples, min(1000,len(valid_examples)))\n",
    "            valid_features = convert_examples_to_features(valid_examples, tokenizer,stage='test')\n",
    "            all_source_ids = torch.tensor([f.source_ids for f in valid_features], dtype=torch.long)\n",
    "            all_source_mask = torch.tensor([f.source_mask for f in valid_features], dtype=torch.long)    \n",
    "            valid_data = TensorDataset(all_source_ids, all_source_mask)   \n",
    "            valid_dataset['valid_bleu']= valid_examples, valid_data\n",
    "\n",
    "        \n",
    "        valid_sampler = SequentialSampler(valid_data)\n",
    "        valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)\n",
    "\n",
    "        model.eval() \n",
    "        p=[]\n",
    "        for batch in valid_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            source_ids,source_mask= batch                  \n",
    "            with torch.no_grad():\n",
    "                preds = model(source_ids=source_ids, source_mask=source_mask)  \n",
    "                for pred in preds:\n",
    "                    t=pred[0].cpu().numpy()\n",
    "                    t=list(t)\n",
    "                    if 0 in t:\n",
    "                        t=t[:t.index(0)]\n",
    "                    text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "                    p.append(text)\n",
    "        model.train()\n",
    "        predictions=[]\n",
    "        with open(os.path.join(output_dir,\"valid.output\"),'w') as f, open(os.path.join(output_dir,\"valid.gold\"),'w') as f1:\n",
    "            for ref, gold in zip(p, valid_examples):\n",
    "                predictions.append(str(gold.idx)+'\\t'+ref)\n",
    "                f.write(str(gold.idx)+'\\t'+ref+'\\n')\n",
    "                f1.write(str(gold.idx)+'\\t'+gold.target+'\\n')     \n",
    "\n",
    "        (goldMap, predictionMap) = bleu.computeMaps(predictions, os.path.join(output_dir, \"valid.gold\")) \n",
    "        valid_bleu=round(bleu.bleuFromMaps(goldMap, predictionMap)[0],2)\n",
    "        msgr.print_msg(\"  {} = {}\".format(\"bleu-4\", str(valid_bleu)))\n",
    "        msgr.print_msg(\"  \"+\"*\"*20)    \n",
    "        if valid_bleu>best_bleu:\n",
    "            msgr.print_msg(\"  Best bleu:{}\".format(valid_bleu))\n",
    "            msgr.print_msg(\"  \"+\"*\"*20)\n",
    "            best_bleu=valid_bleu\n",
    "            # Save best checkpoint for best bleu\n",
    "            best_output_dir = os.path.join(output_dir, 'checkpoint-best-bleu')\n",
    "            if not os.path.exists(best_output_dir):\n",
    "                os.makedirs(best_output_dir)\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "            output_model_file = os.path.join(best_output_dir, \"pytorch_model.bin\")\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_examples = read_examples(test_url)\n",
    "test_features = convert_examples_to_features(test_examples, tokenizer, stage='test')\n",
    "all_source_ids = torch.tensor([f.source_ids for f in test_features], dtype=torch.long)\n",
    "all_source_mask = torch.tensor([f.source_mask for f in test_features], dtype=torch.long)    \n",
    "test_data = TensorDataset(all_source_ids,all_source_mask)   \n",
    "\n",
    "# Calculate bleu\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "model.eval() \n",
    "p=[]\n",
    "for batch in tqdm(test_dataloader,total=len(test_dataloader)):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    source_ids,source_mask= batch                  \n",
    "    with torch.no_grad():\n",
    "        preds = model(source_ids=source_ids,source_mask=source_mask)  \n",
    "        for pred in preds:\n",
    "            t=pred[0].cpu().numpy()\n",
    "            t=list(t)\n",
    "            if 0 in t:\n",
    "                t=t[:t.index(0)]\n",
    "            text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "            p.append(text)\n",
    "model.train()\n",
    "predictions=[]\n",
    "with open(os.path.join(output_dir,\"test.output\"),'w') as f, open(os.path.join(output_dir,\"test.gold\"),'w') as f1:\n",
    "    for ref,gold in zip(p,test_examples):\n",
    "        predictions.append(str(gold.idx)+'\\t'+ref)\n",
    "        f.write(str(gold.idx)+'\\t'+ref+'\\n')\n",
    "        f1.write(str(gold.idx)+'\\t'+gold.target+'\\n')     \n",
    "\n",
    "(goldMap, predictionMap) = bleu.computeMaps(predictions, os.path.join(output_dir, \"test.gold\")) \n",
    "dev_bleu=round(bleu.bleuFromMaps(goldMap, predictionMap)[0],2)\n",
    "msgr.print_msg(\" {} = {} \".format(\"bleu-4\", str(dev_bleu)))\n",
    "msgr.print_msg(\"  \"+\"*\"*20)                      "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "output_dir = exp_dir\n",
    "best_model = output_dir + '/checkpoint-best-ppl/pytorch_model.bin' \n",
    "last_model = output_dir + '/checkpoint-last/pytorch_model.bin' "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.load_state_dict(torch.load(best_model))\n",
    "model.eval() \n",
    "p=[]\n",
    "for batch in tqdm(test_dataloader,total=len(test_dataloader)):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    source_ids,source_mask= batch                  \n",
    "    with torch.no_grad():\n",
    "        preds = model(source_ids=source_ids,source_mask=source_mask)  \n",
    "        for pred in preds:\n",
    "            t=pred[0].cpu().numpy()\n",
    "            t=list(t)\n",
    "            if 0 in t:\n",
    "                t=t[:t.index(0)]\n",
    "            text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "            p.append(text)\n",
    "model.train()\n",
    "predictions=[]\n",
    "with open(os.path.join(output_dir,\"test.output\"),'w') as f, open(os.path.join(output_dir,\"test.gold\"),'w') as f1:\n",
    "    for ref,gold in zip(p,test_examples):\n",
    "        predictions.append(str(gold.idx)+'\\t'+ref)\n",
    "        f.write(str(gold.idx)+'\\t'+ref+'\\n')\n",
    "        f1.write(str(gold.idx)+'\\t'+gold.target+'\\n')     \n",
    "\n",
    "(goldMap, predictionMap) = bleu.computeMaps(predictions, os.path.join(output_dir, \"test.gold\")) \n",
    "dev_bleu=round(bleu.bleuFromMaps(goldMap, predictionMap)[0],2)\n",
    "msgr.print_msg(\" {} = {} \".format(\"bleu-4\", str(dev_bleu)))\n",
    "msgr.print_msg(\"  \"+\"*\"*20)      "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}